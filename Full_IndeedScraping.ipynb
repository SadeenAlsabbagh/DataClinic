{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c84f7bb4-4c57-4fb2-956e-1a61ac00d4b5",
      "metadata": {
        "id": "c84f7bb4-4c57-4fb2-956e-1a61ac00d4b5"
      },
      "outputs": [],
      "source": [
        "from seleniumbase import Driver\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
        "import mysql.connector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "662ada10-7061-4f7a-8944-a8597decc562",
      "metadata": {
        "id": "662ada10-7061-4f7a-8944-a8597decc562"
      },
      "outputs": [],
      "source": [
        "mydb = mysql.connector.connect(\n",
        "    host = \"media-studies-jobs.cux1s0fa60hj.us-east-2.rds.amazonaws.com\",\n",
        "    user = \"admin\",\n",
        "    password = \"123456789\",\n",
        "    database = \"indeed_jobs\"\n",
        ")\n",
        "\n",
        "cursor = mydb.cursor()\n",
        "cursor = mydb.cursor(buffered=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "965f72f7-a71d-48da-8254-fcfef0332d9b",
      "metadata": {
        "id": "965f72f7-a71d-48da-8254-fcfef0332d9b"
      },
      "outputs": [],
      "source": [
        "driver = Driver(uc = True)\n",
        "\n",
        "# Setup explicit wait\n",
        "wait = WebDriverWait(driver, 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03a282b1-6024-4953-be1d-f94783fbbc9d",
      "metadata": {
        "id": "03a282b1-6024-4953-be1d-f94783fbbc9d",
        "outputId": "08762c4d-2092-4092-d3a6-264aad2f660e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching: https://www.indeed.com/jobs?q=media+jobs&l=USA&sc=0kf%3Aexplvl%28ENTRY_LEVEL%29%3B&vjk=1fc92fe6994b5ab5=0\n",
            "033d97870c791106\n",
            "dce55d2f62a20b91\n",
            "Skip job:  d584837266d8bd22\n",
            "Skip job:  736a0f32a2162d73\n",
            "d0e7422f1a79b5ec\n",
            "ca0b90b3fe33e059\n",
            "Skip job:  a1308eaab615e973\n",
            "Skip job:  9657f22ef8886cd9\n",
            "Skip job:  439d442194a11ea3\n",
            "link https://www.indeed.com/viewjob?cmp=eScribers%2C-LLC&t=Proofreader&jk=753b83d2d75001b8&xpse=SoDK67I3CI8PcwzapZ0LbzkdCdPP&xkcb=SoBx67M3CI7zOfwYl50CbzkdCdPP&vjs=3\n",
            "Skip job:  Not listed\n",
            "link https://www.indeed.com/viewjob?cmp=International-College-Counselors&t=Editor&jk=b39d1ec94b4e4770&xpse=SoCr67I3CI8O19SQLJ0LbzkdCdPP&xkcb=SoDs67M3CI7zOfwYl50BbzkdCdPP&vjs=3\n",
            "Skip job:  Not listed\n",
            "link https://www.indeed.com/viewjob?cmp=Good-Thomas%2527-Entertainment&t=Social+Media+Manager&jk=9b08bdea179c2096&xpse=SoC467I3CI8OtOwcyB0LbzkdCdPP&xkcb=SoBY67M3CI7zOfwYl50AbzkdCdPP&vjs=3\n",
            "Skip job:  Not listed\n",
            "Skip job:  2e410a44bfbef628\n",
            "Skip job:  0f52533149e31228\n",
            "link https://www.indeed.com/viewjob?cmp=Juice-Media&t=Content+Writer&jk=bdb0ec41d3478ffe&xpse=SoCT67I3CI8NQvQ33x0LbzkdCdPP&xkcb=SoD_67M3CI7zOfwYl50FbzkdCdPP&vjs=3\n",
            "Skip job:  Not listed\n",
            "Page 1 done.\n",
            "Fetching: https://www.indeed.com/jobs?q=media+jobs&l=USA&sc=0kf%3Aexplvl%28ENTRY_LEVEL%29%3B&vjk=1fc92fe6994b5ab5=12\n",
            "Timed out waiting for page to load\n",
            "Data saved to database successfully.\n"
          ]
        }
      ],
      "source": [
        "jobs_data = []\n",
        "\n",
        "page = 0  # Initialize page counter\n",
        "while True:  # Change to an infinite loop\n",
        "    page_val = page * 12  # Assuming 10 is the pagination step\n",
        "    url = f\"https://www.indeed.com/jobs?q=environmental+science&l=USA&from=searchOnHP&vjk=3242f2585ff64d12{page_val}\"\n",
        "    print(\"Fetching:\", url)\n",
        "\n",
        "    driver.get(url)\n",
        "\n",
        "    # Wait for the job listings to be loaded\n",
        "    try:\n",
        "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.job_seen_beacon')))\n",
        "    except TimeoutException:\n",
        "        print(\"Timed out waiting for page to load\")\n",
        "        break  # Break the loop if the page doesn't load\n",
        "\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    jobs = soup.findAll('div', class_='job_seen_beacon')\n",
        "\n",
        "    if not jobs:\n",
        "        print(\"No more jobs found. Ending search.\")\n",
        "        break  # Break the loop if no jobs are found\n",
        "\n",
        "    for job in jobs:\n",
        "        data = {}\n",
        "\n",
        "        job_link_element = job.find('a', href=True)\n",
        "        job_link = 'https://www.indeed.com' + job_link_element['href'] if job_link_element else \"Not listed\"\n",
        "        data['Apply_Link'] = job_link\n",
        "\n",
        "        # Visit the job page to extract detailed description\n",
        "        if job_link != \"Not listed\":\n",
        "            driver.get(job_link)\n",
        "            try:\n",
        "                link = driver.current_url\n",
        "                if \"?jk=\" in link:\n",
        "                    try:\n",
        "                        job_id = link.split('?jk=')[1].split('&')[0]\n",
        "                    except IndexError:\n",
        "                        job_id = \"Error extracting ID\"\n",
        "                else:\n",
        "                    print('link', link)\n",
        "                    job_id = \"Not listed\"\n",
        "\n",
        "                    try:\n",
        "                        title = job.find('h2', class_='jobTitle').text.strip()\n",
        "                    except AttributeError:\n",
        "                        title = \"Not listed\"\n",
        "                    data['Title'] = title\n",
        "\n",
        "                    try:\n",
        "                        company = job.find('span', class_=\"css-92r8pb\").text.strip()\n",
        "                    except AttributeError:\n",
        "                        company = \"Not listed\"\n",
        "                    data['Company'] = company\n",
        "\n",
        "                    try:\n",
        "                        location = job.find('div', class_='css-1p0sjhy').text.strip()\n",
        "                    except AttributeError:\n",
        "                        location = \"Not listed\"\n",
        "                    data['Location'] = location\n",
        "\n",
        "                    salary = job.find('div', class_='css-1cvo3fd')\n",
        "                    data['Salary'] = salary.text.strip() if salary else \"Not listed\"\n",
        "\n",
        "                    try:\n",
        "                      wait.until(EC.presence_of_element_located((By.ID, 'jobDescriptionText')))\n",
        "                      detailed_description = driver.find_element(By.ID, 'jobDescriptionText').text.strip()\n",
        "                    except (TimeoutException, NoSuchElementException):\n",
        "                      detailed_description = \"Not listed\"\n",
        "\n",
        "                    data['Description'] = detailed_description\n",
        "                    jobs_data.append(data)\n",
        "\n",
        "    print(f\"Page {page + 1} done.\")\n",
        "    page += 1  # Increment the page counter\n",
        "\n",
        "# Close the WebDriver after finishing\n",
        "driver.quit()\n",
        "\n",
        "# Save data to CSV file\n",
        "csv_file_path = \"jobsindeed.csv\"\n",
        "with open(csv_file_path, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "    fieldnames = ['Title', 'Company', 'Location', 'Salary', 'Description', 'Apply_Link']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for job_data in jobs_data:\n",
        "        writer.writerow(job_data)\n",
        "\n",
        "print(\"Data saved to CSV file successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca718828-7efe-4b57-bb36-3a860003992f",
      "metadata": {
        "id": "ca718828-7efe-4b57-bb36-3a860003992f"
      },
      "outputs": [],
      "source": [
        "jobs_data = []\n",
        "\n",
        "page = 0  # Initialize page counter\n",
        "while True:  # Change to an infinite loop\n",
        "    page_val = page * 12  # Assuming 10 is the pagination step\n",
        "    url = f\"https://www.indeed.com/jobs?q=environmental+science&l=USA&from=searchOnHP&vjk=3242f2585ff64d12{page_val}\"\n",
        "    print(\"Fetching:\", url)\n",
        "\n",
        "    driver.get(url)\n",
        "\n",
        "    # Wait for the job listings to be loaded\n",
        "    try:\n",
        "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.job_seen_beacon')))\n",
        "    except TimeoutException:\n",
        "        print(\"Timed out waiting for page to load\")\n",
        "        break  # Break the loop if the page doesn't load\n",
        "\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    jobs = soup.findAll('div', class_='job_seen_beacon')\n",
        "\n",
        "    if not jobs:\n",
        "        print(\"No more jobs found. Ending search.\")\n",
        "        break  # Break the loop if no jobs are found\n",
        "\n",
        "    for job in jobs:\n",
        "        data = {}\n",
        "\n",
        "        job_link_element = job.find('a', href=True)\n",
        "        job_link = 'https://www.indeed.com' + job_link_element['href'] if job_link_element else \"Not listed\"\n",
        "        data['Apply_Link'] = job_link\n",
        "\n",
        "        # Visit the job page to extract detailed description\n",
        "        if job_link != \"Not listed\":\n",
        "            driver.get(job_link)\n",
        "            try:\n",
        "                link = driver.current_url\n",
        "                if \"?jk=\" in link:\n",
        "                    try:\n",
        "                        job_id = link.split('?jk=')[1].split('&')[0]\n",
        "                    except IndexError:\n",
        "                        job_id = \"Error extracting ID\"\n",
        "                else:\n",
        "                    print('link', link)\n",
        "                    job_id = \"Not listed\"\n",
        "\n",
        "                select_query = \"SELECT Job_ID FROM indeed_jobs WHERE Job_ID = %s\"\n",
        "                cursor.execute(select_query, (job_id,))\n",
        "                result = cursor.fetchone()\n",
        "\n",
        "                if result is None:\n",
        "                    data['Job_ID'] = job_id\n",
        "                    print(job_id)\n",
        "\n",
        "                    try:\n",
        "                        title = job.find('h2', class_='jobTitle').text.strip()\n",
        "                    except AttributeError:\n",
        "                        title = \"Not listed\"\n",
        "                    data['Title'] = title\n",
        "\n",
        "                    try:\n",
        "                        company = job.find('span', class_=\"css-92r8pb\").text.strip()\n",
        "                    except AttributeError:\n",
        "                        company = \"Not listed\"\n",
        "                    data['Company'] = company\n",
        "\n",
        "                    try:\n",
        "                        location = job.find('div', class_='css-1p0sjhy').text.strip()\n",
        "                    except AttributeError:\n",
        "                        location = \"Not listed\"\n",
        "                    data['Location'] = location\n",
        "\n",
        "                    salary = job.find('div', class_='css-1cvo3fd')\n",
        "                    data['Salary'] = salary.text.strip() if salary else \"Not listed\"\n",
        "\n",
        "                    try:\n",
        "                      wait.until(EC.presence_of_element_located((By.ID, 'jobDescriptionText')))\n",
        "                      detailed_description = driver.find_element(By.ID, 'jobDescriptionText').text.strip()\n",
        "                    except (TimeoutException, NoSuchElementException):\n",
        "                      detailed_description = \"Not listed\"\n",
        "\n",
        "                    data['Description'] = detailed_description\n",
        "                    jobs_data.append(data)\n",
        "\n",
        "    print(f\"Page {page + 1} done.\")\n",
        "    page += 1  # Increment the page counter\n",
        "\n",
        "# Close the WebDriver after finishing\n",
        "driver.quit()\n",
        "\n",
        "# Save data to CSV file\n",
        "csv_file_path = \"242024.csv\"\n",
        "with open(csv_file_path, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "    fieldnames = ['Title', 'Company', 'Location', 'Salary', 'Description', 'Apply_Link']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for job_data in jobs_data:\n",
        "        writer.writerow(job_data)\n",
        "\n",
        "print(\"Data saved to CSV file successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "jobs_data = []\n",
        "\n",
        "page = 0  # Initialize page counter\n",
        "while page < 5:  # Limit to 5 pages for demonstration purposes\n",
        "    page_val = page * 12  # Assuming 12 is the pagination step\n",
        "    url = f\"https://www.indeed.com/jobs?q=environmental+science&l=USA&from=searchOnHP&vjk=3242f2585ff64d12{page_val}\"\n",
        "    print(\"Fetching:\", url)\n",
        "\n",
        "    driver.get(url)\n",
        "\n",
        "    # Wait for the job listings to be loaded\n",
        "    try:\n",
        "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.jobsearch-SerpJobCard')))\n",
        "    except TimeoutException:\n",
        "        print(\"Timed out waiting for page to load\")\n",
        "        break  # Break the loop if the page doesn't load\n",
        "\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    jobs = soup.find_all('div', class_='jobsearch-SerpJobCard')\n",
        "\n",
        "    if not jobs:\n",
        "        print(\"No more jobs found. Ending search.\")\n",
        "        break  # Break the loop if no jobs are found\n",
        "\n",
        "    for job in jobs:\n",
        "        data = {}\n",
        "\n",
        "        job_link_element = job.find('a', href=True)\n",
        "        job_link = 'https://www.indeed.com' + job_link_element['href'] if job_link_element else \"Not listed\"\n",
        "        data['Apply_Link'] = job_link\n",
        "\n",
        "        # Visit the job page to extract detailed description\n",
        "        if job_link != \"Not listed\":\n",
        "            driver.get(job_link)\n",
        "            try:\n",
        "                job_id = job_link.split('?jk=')[1].split('&')[0]\n",
        "            except IndexError:\n",
        "                job_id = \"Error extracting ID\"\n",
        "\n",
        "            data['Job_ID'] = job_id\n",
        "\n",
        "            try:\n",
        "                title = job.find('h2', class_='title').text.strip()\n",
        "            except AttributeError:\n",
        "                title = \"Not listed\"\n",
        "            data['Title'] = title\n",
        "\n",
        "            try:\n",
        "                company = job.find('span', class_=\"company\").text.strip()\n",
        "            except AttributeError:\n",
        "                company = \"Not listed\"\n",
        "            data['Company'] = company\n",
        "\n",
        "            try:\n",
        "                location = job.find('div', class_='location').text.strip()\n",
        "            except AttributeError:\n",
        "                location = \"Not listed\"\n",
        "            data['Location'] = location\n",
        "\n",
        "            salary = job.find('span', class_='salaryText')\n",
        "            data['Salary'] = salary.text.strip() if salary else \"Not listed\"\n",
        "\n",
        "            try:\n",
        "                detailed_description = driver.find_element(By.ID, 'jobDescriptionText').text.strip()\n",
        "            except (NoSuchElementException, TimeoutException):\n",
        "                detailed_description = \"Not listed\"\n",
        "\n",
        "            data['Description'] = detailed_description\n",
        "            jobs_data.append(data)\n",
        "\n",
        "    print(f\"Page {page + 1} done.\")\n",
        "    page += 1  # Increment the page counter\n",
        "\n",
        "# Close the WebDriver after finishing\n",
        "driver.quit()\n",
        "\n",
        "# Save data to CSV file\n",
        "csv_file_path = \"242024.csv\"\n",
        "with open(csv_file_path, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "    fieldnames = ['Title', 'Company', 'Location', 'Salary', 'Description', 'Apply_Link']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for job_data in jobs_data:\n",
        "        writer.writerow(job_data)\n",
        "\n",
        "print(\"Data saved to CSV file successfully.\")"
      ],
      "metadata": {
        "id": "tja4ByMGRjzx"
      },
      "id": "tja4ByMGRjzx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Works fine with no incrimnation"
      ],
      "metadata": {
        "id": "Ove7Lem1MhUF"
      },
      "id": "Ove7Lem1MhUF"
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Initialize list to store job data\n",
        "jobs_data = []\n",
        "\n",
        "max_pages = 200\n",
        "page = 0  # Initialize page counter\n",
        "while page < max_pages:  # Limit the loop to 200 pages\n",
        "    page_val = page * 12  # Assuming 10 is the pagination step\n",
        "    url = f\"https://www.indeed.com/jobs?q=environmental+science&l=USA&from=searchOnHP&vjk=3242f2585ff64d12{page_val}\"\n",
        "    print(\"Fetching:\", url)\n",
        "\n",
        "    driver.get(url)\n",
        "\n",
        "    # Wait for the job listings to be loaded\n",
        "    try:\n",
        "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.job_seen_beacon')))\n",
        "    except TimeoutException:\n",
        "        print(\"Timed out waiting for page to load\")\n",
        "        continue  # Skip to the next iteration if the page doesn't load\n",
        "\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    jobs = soup.findAll('div', class_='job_seen_beacon')\n",
        "\n",
        "    if not jobs:\n",
        "        print(\"No more jobs found. Ending search.\")\n",
        "        break  # Break the loop if no jobs are found\n",
        "\n",
        "    for job in jobs:\n",
        "        data = {}\n",
        "\n",
        "        job_link_element = job.find('a', href=True)\n",
        "        job_link = 'https://www.indeed.com' + job_link_element['href'] if job_link_element else \"Not listed\"\n",
        "        data['Apply_Link'] = job_link\n",
        "\n",
        "        # Visit the job page to extract detailed description\n",
        "        if job_link != \"Not listed\":\n",
        "            driver.get(job_link)\n",
        "            try:\n",
        "                link = driver.current_url\n",
        "                if \"?jk=\" in link:\n",
        "                    try:\n",
        "                        job_id = link.split('?jk=')[1].split('&')[0]\n",
        "                    except IndexError:\n",
        "                        job_id = \"Error extracting ID\"\n",
        "                else:\n",
        "                    print('link', link)\n",
        "                    job_id = \"Not listed\"\n",
        "\n",
        "                if job_id is None:\n",
        "                    data['Job_ID'] = job_id\n",
        "                    print(job_id)\n",
        "\n",
        "                    try:\n",
        "                        title = job.find('h2', class_='jobTitle').text.strip()\n",
        "                    except AttributeError:\n",
        "                        title = \"Not listed\"\n",
        "                    data['Title'] = title\n",
        "\n",
        "                    try:\n",
        "                        company = job.find('span', class_=\"css-92r8pb\").text.strip()\n",
        "                    except AttributeError:\n",
        "                        company = \"Not listed\"\n",
        "                    data['Company'] = company\n",
        "\n",
        "                    try:\n",
        "                        location = job.find('div', class_='css-1p0sjhy').text.strip()\n",
        "                    except AttributeError:\n",
        "                        location = \"Not listed\"\n",
        "                    data['Location'] = location\n",
        "\n",
        "                    salary = job.find('div', class_='css-1cvo3fd')\n",
        "                    data['Salary'] = salary.text.strip() if salary else \"Not listed\"\n",
        "\n",
        "                    try:\n",
        "                        detailed_description = job.find('div', class_='summary').text.strip()\n",
        "                    except AttributeError:\n",
        "                        detailed_description = \"Not listed\"\n",
        "                    data['Description'] = detailed_description\n",
        "\n",
        "                else:\n",
        "                    print(\"Skip job: \", job_id)\n",
        "\n",
        "            except (TimeoutException, NoSuchElementException):\n",
        "                detailed_description = \"Not listed\"\n",
        "                data['Description'] = detailed_description\n",
        "\n",
        "            jobs_data.append(data)\n",
        "\n",
        "    print(f\"Page {page + 1} done.\")\n",
        "    page += 1  # Increment the page counter\n",
        "\n",
        "# Close the WebDriver after finishing\n",
        "driver.quit()\n",
        "\n",
        "# Save data to CSV file\n",
        "csv_file_path = \"fuckedup.csv\"\n",
        "with open(csv_file_path, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "    fieldnames = ['Title', 'Company', 'Location', 'Salary', 'Description', 'Apply_Link']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for job_data in jobs_data:\n",
        "        writer.writerow(job_data)\n",
        "\n",
        "print(\"Data saved to CSV file successfully.\")\n"
      ],
      "metadata": {
        "id": "hI6A779_LSbR"
      },
      "id": "hI6A779_LSbR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 max page, multiple URLs"
      ],
      "metadata": {
        "id": "2GO4_uGTMjl_"
      },
      "id": "2GO4_uGTMjl_"
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Initialize list to store job data\n",
        "jobs_data = []\n",
        "\n",
        "max_pages = 1\n",
        "page = 0  # Initialize page counter\n",
        "while page < max_pages:\n",
        "    page_val = page * 12  # Assuming 10 is the pagination step\n",
        "    url = f\"https://www.indeed.com/jobs?q=environmental+science&l=USA&from=searchOnHP&vjk=3242f2585ff64d12{page_val}\"\n",
        "        url = f\"https://www.indeed.com/jobs?q=sustainability+jobs&l=USA&sc=0kf%3Aexplvl%28ENTRY_LEVEL%29%3B&vjk=b8a5956f0918959c{page_val}\"\n",
        "    url = f\"https://www.indeed.com/jobs?q=sustainable+agriculture&l=USA&vjk=b9b8bf61620566e1{page_val}\"\n",
        "    url = f\"https://www.indeed.com/jobs?q=sustainability+consultant&l=USA&vjk=4709ed414e838017{page_val}\"\n",
        "\n",
        "    print(\"Fetching:\", url)\n",
        "\n",
        "    driver.get(url)\n",
        "\n",
        "    # Wait for the job listings to be loaded\n",
        "    try:\n",
        "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.job_seen_beacon')))\n",
        "    except TimeoutException:\n",
        "        print(\"Timed out waiting for page to load\")\n",
        "        continue  # Skip to the next iteration if the page doesn't load\n",
        "\n",
        "print(f\"Page {page + 1} done.\")\n",
        "page += 1  # Increment the page counter\n",
        "\n",
        "\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    jobs = soup.findAll('div', class_='job_seen_beacon')\n",
        "\n",
        "    if not jobs:\n",
        "        print(\"No more jobs found. Ending search.\")\n",
        "        break  # Break the loop if no jobs are found\n",
        "\n",
        "    for job in jobs:\n",
        "        data = {}\n",
        "\n",
        "        job_link_element = job.find('a', href=True)\n",
        "        job_link = 'https://www.indeed.com' + job_link_element['href'] if job_link_element else \"Not listed\"\n",
        "        data['Apply_Link'] = job_link\n",
        "\n",
        "        # Visit the job page to extract detailed description\n",
        "        if job_link != \"Not listed\":\n",
        "            driver.get(job_link)\n",
        "            try:\n",
        "                link = driver.current_url\n",
        "                if \"?jk=\" in link:\n",
        "                    try:\n",
        "                        job_id = link.split('?jk=')[1].split('&')[0]\n",
        "                    except IndexError:\n",
        "                        job_id = \"Error extracting ID\"\n",
        "                else:\n",
        "                    print('link', link)\n",
        "                    job_id = \"Not listed\"\n",
        "\n",
        "                if job_id is None:\n",
        "                    data['Job_ID'] = job_id\n",
        "                    print(job_id)\n",
        "\n",
        "                    try:\n",
        "                        title = job.find('h2', class_='jobTitle').text.strip()\n",
        "                    except AttributeError:\n",
        "                        title = \"Not listed\"\n",
        "                    data['Title'] = title\n",
        "\n",
        "                    try:\n",
        "                        company = job.find('span', class_=\"css-92r8pb\").text.strip()\n",
        "                    except AttributeError:\n",
        "                        company = \"Not listed\"\n",
        "                    data['Company'] = company\n",
        "\n",
        "                    try:\n",
        "                        location = job.find('div', class_='css-1p0sjhy').text.strip()\n",
        "                    except AttributeError:\n",
        "                        location = \"Not listed\"\n",
        "                    data['Location'] = location\n",
        "\n",
        "                    salary = job.find('div', class_='css-1cvo3fd')\n",
        "                    data['Salary'] = salary.text.strip() if salary else \"Not listed\"\n",
        "\n",
        "                    try:\n",
        "                        detailed_description = job.find('div', class_='summary').text.strip()\n",
        "                    except AttributeError:\n",
        "                        detailed_description = \"Not listed\"\n",
        "                    data['Description'] = detailed_description\n",
        "\n",
        "                else:\n",
        "                    print(\"Skip job: \", job_id)\n",
        "\n",
        "            except (TimeoutException, NoSuchElementException):\n",
        "                detailed_description = \"Not listed\"\n",
        "                data['Description'] = detailed_description\n",
        "\n",
        "            jobs_data.append(data)\n",
        "\n",
        "\n",
        "# Close the WebDriver after finishing\n",
        "driver.quit()\n",
        "\n",
        "# Save data to CSV file\n",
        "csv_file_path = \"fuckthis.csv\"\n",
        "with open(csv_file_path, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "    fieldnames = ['Title', 'Company', 'Location', 'Salary', 'Description', 'Apply_Link']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for job_data in jobs_data:\n",
        "        writer.writerow(job_data)\n",
        "\n",
        "print(\"Data saved to CSV file successfully.\")\n"
      ],
      "metadata": {
        "id": "lRAgH-p9MmCt"
      },
      "id": "lRAgH-p9MmCt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xO_67Mw_TQYZ"
      },
      "id": "xO_67Mw_TQYZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fCQy1dabTQVf"
      },
      "id": "fCQy1dabTQVf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Setup Chrome options for Selenium\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "# Uncomment the next line to run Chrome in headless mode\n",
        "# options.add_argument('--headless')\n",
        "\n",
        "# Initialize the Chrome WebDriver\n",
        "driver = webdriver.Chrome(options=options)\n",
        "\n",
        "# Setup explicit wait\n",
        "wait = WebDriverWait(driver, 10)\n",
        "\n",
        "jobs_data = []\n",
        "max_pages = 5  # Set a maximum number of pages to scrape\n",
        "page = 0\n",
        "\n",
        "while page < max_pages:\n",
        "    url = f\"https://www.indeed.com/jobs?q=sustainability++jobs&l=USA&vjk={page * 10}\"\n",
        "    driver.get(url)\n",
        "\n",
        "    #https://www.indeed.com/jobs?q=sustainability++jobs&l=USA&vjk=0cdb55029eb47b25\n",
        "\n",
        "    try:\n",
        "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.job_seen_beacon')))\n",
        "    except TimeoutException:\n",
        "        print(\"Timed out waiting for page to load\")\n",
        "        break\n",
        "\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    jobs = soup.findAll('div', class_='job_seen_beacon')\n",
        "\n",
        "    if not jobs:\n",
        "        print(\"No more jobs found.\")\n",
        "        break\n",
        "\n",
        "    for job in jobs:\n",
        "        data = {}\n",
        "        try:\n",
        "            data['Title'] = job.find('h2', class_='jobTitle').text.strip()\n",
        "        except AttributeError:\n",
        "            data['Title'] = \"Not listed\"\n",
        "\n",
        "        try:\n",
        "            data['Company'] = job.find('span', class_=\"css-92r8pb\").text.strip()\n",
        "        except AttributeError:\n",
        "            data['Company'] = \"Not listed\"\n",
        "\n",
        "        try:\n",
        "            data['Location'] = job.find('div', class_='company_location').text.strip()\n",
        "        except AttributeError:\n",
        "            data['Location'] = \"Not listed\"\n",
        "\n",
        "        skills_section = job.find('div', class_='jobsearch-SkillsWrapper')\n",
        "        data['Skills'] = ', '.join([skill.text.strip() for skill in skills_section.find_all('span', class_='jobsearch-SkillTag')]) if skills_section else \"Not listed\"\n",
        "\n",
        "        try:\n",
        "            data['Salary'] = job.find('div', class_='css-1cvo3fd').text.strip()\n",
        "        except AttributeError:\n",
        "            data['Salary'] = \"Not listed\"\n",
        "\n",
        "        try:\n",
        "            jobDescriptionText = job.find('div', class_='jobsearch-jobDescriptionText')\n",
        "            data['Description'] = jobDescriptionText.text.strip() if jobDescriptionText else \"Not listed\"\n",
        "        except AttributeError:\n",
        "            data['Description'] = \"Not listed\"\n",
        "\n",
        "        try:\n",
        "            job_link = job.find('a', class_='jcs-JobTitle')['href']\n",
        "            data['Apply Link'] = 'https://www.indeed.com' + job_link\n",
        "        except TypeError:\n",
        "            data['Apply Link'] = \"Not listed\"\n",
        "\n",
        "        jobs_data.append(data)\n",
        "\n",
        "    print(f\"Page {page + 1} completed.\")\n",
        "    page += 1\n",
        "\n",
        "driver.quit()\n",
        "\n",
        "df = pd.DataFrame(jobs_data)\n",
        "df.to_csv('indeed_jobs.csv', index=False)\n",
        "print(\"Job extraction complete. Data saved to 'indeed_jobs.csv'.\")"
      ],
      "metadata": {
        "id": "trzPdjQ7TQTB"
      },
      "id": "trzPdjQ7TQTB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jobs_data = []\n",
        "\n",
        "\n",
        "for i in range(0,2000,10):\n",
        "    driver.get('https://www.indeed.com/jobs?q=sustainability%20&l=united%20states&start='+str(i))\n",
        "    driver.implicitly_wait(5)\n",
        "\n",
        "    #https://www.indeed.com/jobs?q=sustainability++jobs&l=USA&vjk=0cdb55029eb47b25\n",
        "\n",
        "    try:\n",
        "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.job_seen_beacon')))\n",
        "    except TimeoutException:\n",
        "        print(\"Timed out waiting for page to load\")\n",
        "        break\n",
        "\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    jobs = soup.findAll('div', class_='job_seen_beacon')\n",
        "\n",
        "    if not jobs:\n",
        "        print(\"No more jobs found.\")\n",
        "        break\n",
        "\n",
        "    for job in jobs:\n",
        "        data = {}\n",
        "\n",
        "\n",
        "        try:\n",
        "          wait.until(EC.presence_of_element_located((By.ID, 'jobDescriptionText')))\n",
        "          detailed_description = driver.find_element(By.ID, 'jobDescriptionText').text.strip()\n",
        "        except (TimeoutException, NoSuchElementException):\n",
        "          detailed_description = \"Not listed\"\n",
        "\n",
        "          data['Description'] = detailed_description\n",
        "\n",
        "        try:\n",
        "            data['Title'] = job.find('h2', class_='jobTitle').text.strip()\n",
        "        except AttributeError:\n",
        "            data['Title'] = \"Not listed\"\n",
        "\n",
        "        try:\n",
        "            data['Company'] = job.find('span', class_=\"css-92r8pb\").text.strip()\n",
        "        except AttributeError:\n",
        "            data['Company'] = \"Not listed\"\n",
        "\n",
        "        try:\n",
        "            data['Location'] = job.find('div', class_='company_location').text.strip()\n",
        "        except AttributeError:\n",
        "            data['Location'] = \"Not listed\"\n",
        "\n",
        "        skills_section = job.find('div', class_='jobsearch-SkillsWrapper')\n",
        "        data['Skills'] = ', '.join([skill.text.strip() for skill in skills_section.find_all('span', class_='jobsearch-SkillTag')]) if skills_section else \"Not listed\"\n",
        "\n",
        "        try:\n",
        "            data['Salary'] = job.find('div', class_='css-1cvo3fd').text.strip()\n",
        "        except AttributeError:\n",
        "            data['Salary'] = \"Not listed\"\n",
        "\n",
        "\n",
        "        try:\n",
        "            wait.until(EC.presence_of_element_located((By.ID, 'jobDescriptionText')))\n",
        "            detailed_description = driver.find_element(By.ID, 'jobDescriptionText').text.strip()\n",
        "        except (TimeoutException, NoSuchElementException):\n",
        "            detailed_description = \"Not listed\"\n",
        "\n",
        "        data['Description'] = detailed_description\n",
        "\n",
        "\n",
        "        try:\n",
        "            job_link = job.find('a', class_='jcs-JobTitle')['href']\n",
        "            data['Apply Link'] = 'https://www.indeed.com' + job_link\n",
        "        except TypeError:\n",
        "            data['Apply Link'] = \"Not listed\"\n",
        "\n",
        "        jobs_data.append(data)\n",
        "        page += 1\n",
        "    print(f\"Page {page + 1} completed.\")\n",
        "\n",
        "\n",
        "driver.quit()\n",
        "\n",
        "df = pd.DataFrame(jobs_data)\n",
        "df.to_csv('clean_scrapped.csv', index=False)\n",
        "print(\"Job extraction complete. Data saved to 'indeed_jobs.csv'.\")"
      ],
      "metadata": {
        "id": "G1HCo5m4Ynxk"
      },
      "id": "G1HCo5m4Ynxk",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}