{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c84f7bb4-4c57-4fb2-956e-1a61ac00d4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seleniumbase import Driver\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "import mysql.connector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "662ada10-7061-4f7a-8944-a8597decc562",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydb = mysql.connector.connect(\n",
    "    host = \"media-studies-jobs.cux1s0fa60hj.us-east-2.rds.amazonaws.com\",\n",
    "    user = \"admin\",\n",
    "    password = \"123456789\",\n",
    "    database = \"indeed_jobs\"\n",
    ")\n",
    "\n",
    "cursor = mydb.cursor()\n",
    "cursor = mydb.cursor(buffered=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "965f72f7-a71d-48da-8254-fcfef0332d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = Driver(uc = True)\n",
    "\n",
    "# Setup explicit wait\n",
    "wait = WebDriverWait(driver, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "354a2bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09aa7f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.indeed.com/jobs?q=environmental+science&l=USA&from=searchOnHP&vjk=3242f2585ff64d120\n",
      "Page 1 done.\n",
      "Skip job:  100cf9ab0d80fbfd\n",
      "Skip job:  d9b504095622a5d2\n",
      "Skip job:  69b197e5362f788c\n",
      "Skip job:  10914963c7691226\n",
      "Skip job:  15b03c439db0492f\n",
      "Skip job:  2e1cb9dff67c1180\n",
      "Skip job:  2db42389ad4c0198\n",
      "link https://www.indeed.com/viewjob?cmp=Insightful-Tutors&t=Test+Preparation+Tutor&jk=9fbde373fa3613d2&xpse=SoAg67I3CPgXBMSdbh0LbzkdCdPP&xkcb=SoCt67M3CPgaVLTaqJ0MbzkdCdPP&vjs=3\n",
      "Skip job:  Not listed\n",
      "Skip job:  2582aa991ef46017\n",
      "link https://www.indeed.com/viewjob?cmp=Forest-Carbon-Works&t=Quality+Assurance+Coordinator&jk=b4868c10f0a71ebf&xpse=SoDm67I3CPgWcJQtmx0LbzkdCdPP&xkcb=SoDw67M3CPgaVLTaqJ0CbzkdCdPP&vjs=3\n",
      "Skip job:  Not listed\n",
      "Skip job:  16bcea7a789a883e\n",
      "Skip job:  d436326cd46ad90c\n",
      "link https://www.indeed.com/viewjob?cmp=BrightPath-Associates-LLC&t=Environmental+Scientist&jk=f8152a0e7459efec&xpse=SoDh67I3CPgVPcQuyR0LbzkdCdPP&xkcb=SoBX67M3CPgaVLTaqJ0HbzkdCdPP&vjs=3\n",
      "Skip job:  Not listed\n",
      "Skip job:  cc40729c8f3acb2d\n",
      "Skip job:  8717269607284626\n",
      "Fetching: https://www.indeed.com/jobs?q=sustainability+jobs&l=USA&sc=0kf%3Aexplvl%28ENTRY_LEVEL%29%3B&vjk=b8a5956f0918959c0\n",
      "Page 1 done.\n",
      "Skip job:  c2c8c75d75cbea35\n",
      "Skip job:  7e66b88301cb3ec2\n",
      "Skip job:  93ee0a36bba43879\n",
      "Skip job:  943e58a379cb4ba8\n",
      "Skip job:  73a2d0fc6d309889\n",
      "Skip job:  7d0e546a1fc0d01b\n",
      "Skip job:  cdd066f8cc304084\n",
      "Skip job:  be7aab64ce587b28\n",
      "Skip job:  ab7c3aed8f5e0c42\n",
      "Skip job:  7658e755bf6be8c0\n",
      "Skip job:  f06d3d3e4bf71ca5\n",
      "Skip job:  d673ba9c032d370e\n",
      "Skip job:  a1872b99c40eadd1\n",
      "link https://www.indeed.com/viewjob?cmp=Park-Tutoring&t=Test+Preparation+Tutor&jk=63c91aca873a6388&xpse=SoDq67I3CPgua-SQIh0LbzkdCdPP&xkcb=SoCQ67M3CPgTxJwTBp0GbzkdCdPP&vjs=3\n",
      "Skip job:  Not listed\n",
      "Skip job:  28cfd41249327fc8\n",
      "Fetching: https://www.indeed.com/jobs?q=sustainable+agriculture&l=USA&vjk=b9b8bf61620566e10\n",
      "Page 1 done.\n",
      "Skip job:  7cbccac1a09c973e\n",
      "Skip job:  65acf94078041605\n",
      "Skip job:  6f34b992209848cf\n",
      "Skip job:  1b785f2dcb6934fd\n",
      "Skip job:  0bbfb30301ca70d5\n",
      "Skip job:  1f4118bcc6ba2361\n",
      "Skip job:  23aa39b0e34474f5\n",
      "Skip job:  4d7261138eb9a94d\n",
      "Skip job:  eb9a562a86ca3f7c\n",
      "Skip job:  c1cb1c779cd6e7e6\n",
      "Skip job:  d447750facdc4612\n",
      "Skip job:  8f024a99849064a8\n",
      "Skip job:  38375a32d06a1044\n",
      "Skip job:  9ff6b02f7257eca0\n",
      "Skip job:  de91d448f40910f1\n",
      "Fetching: https://www.indeed.com/jobs?q=sustainability+consultant&l=USA&vjk=4709ed414e8380170\n",
      "Page 1 done.\n",
      "Skip job:  819636bcd3305a34\n",
      "Skip job:  4709ed414e838017\n",
      "Skip job:  0094ed291eeb508a\n",
      "Skip job:  9202638b97f5d4bf\n",
      "Skip job:  91766275a0c3155f\n",
      "Skip job:  a338829fb48abb7f\n",
      "Skip job:  93ee0a36bba43879\n",
      "Skip job:  3ea26f46af33d957\n",
      "Skip job:  bebbab8df025d96a\n",
      "Skip job:  3d2febd265207474\n",
      "Skip job:  b7e9b539d7f98ea4\n",
      "Skip job:  fde37e102f880b16\n",
      "Skip job:  9c1e6eee25196e0d\n",
      "Skip job:  ae21562885ec091e\n",
      "link https://www.indeed.com/viewjob?cmp=IT-First-Source&t=Environmental+Consultant&jk=b853aa849ba11c98&xpse=SoAR67I3CPgg0aSAoB0LbzkdCdPP&xkcb=SoCB67M3CPgnMTR59R0FbzkdCdPP&vjs=3\n",
      "Skip job:  Not listed\n",
      "Data saved to CSV file successfully.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://www.indeed.com/jobs?q=environmental+science&l=USA&from=searchOnHP&vjk=3242f2585ff64d12\",\n",
    "    \"https://www.indeed.com/jobs?q=sustainability+jobs&l=USA&sc=0kf%3Aexplvl%28ENTRY_LEVEL%29%3B&vjk=b8a5956f0918959c\",\n",
    "    \"https://www.indeed.com/jobs?q=sustainable+agriculture&l=USA&vjk=b9b8bf61620566e1\",\n",
    "    \"https://www.indeed.com/jobs?q=sustainability+consultant&l=USA&vjk=4709ed414e838017\"\n",
    "]\n",
    "\n",
    "# Initialize list to store job data\n",
    "jobs_data = []\n",
    "\n",
    "max_pages = 1\n",
    "for url in urls:\n",
    "    page = 0  # Initialize page counter\n",
    "    while page < max_pages:  # Limit the loop to 1 page per URL\n",
    "        page_val = page * 12  # Assuming 10 is the pagination step\n",
    "        full_url = f\"{url}{page_val}\"\n",
    "        print(\"Fetching:\", full_url)\n",
    "\n",
    "        driver.get(full_url)\n",
    "\n",
    "        # Wait for the job listings to be loaded\n",
    "        try:\n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.job_seen_beacon')))\n",
    "        except TimeoutException:\n",
    "            print(\"Timed out waiting for page to load\")\n",
    "            continue  # Skip to the next iteration if the page doesn't load\n",
    "\n",
    "        print(f\"Page {page + 1} done.\")\n",
    "        page += 1  # Increment the page counter\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        jobs = soup.findAll('div', class_='job_seen_beacon')\n",
    "\n",
    "        if not jobs:\n",
    "            print(\"No more jobs found. Ending search.\")\n",
    "            break  # Break the loop if no jobs are found\n",
    "\n",
    "        for job in jobs:\n",
    "            data = {}\n",
    "\n",
    "            job_link_element = job.find('a', href=True)\n",
    "            job_link = 'https://www.indeed.com' + job_link_element['href'] if job_link_element else \"Not listed\"\n",
    "            data['Apply_Link'] = job_link\n",
    "\n",
    "            # Visit the job page to extract detailed description\n",
    "            if job_link != \"Not listed\":\n",
    "                driver.get(job_link)\n",
    "                try:\n",
    "                    link = driver.current_url\n",
    "                    if \"?jk=\" in link:\n",
    "                        try:\n",
    "                            job_id = link.split('?jk=')[1].split('&')[0]\n",
    "                        except IndexError:\n",
    "                            job_id = \"Error extracting ID\"\n",
    "                    else:\n",
    "                        print('link', link)\n",
    "                        job_id = \"Not listed\"\n",
    "\n",
    "                    if job_id is None:\n",
    "                        data['Job_ID'] = job_id\n",
    "                        print(job_id)\n",
    "\n",
    "                        try:\n",
    "                            title = job.find('h2', class_='jobTitle').text.strip()\n",
    "                        except AttributeError:\n",
    "                            title = \"Not listed\"\n",
    "                        data['Title'] = title\n",
    "\n",
    "                        try:\n",
    "                            company = job.find('span', class_=\"css-92r8pb\").text.strip()\n",
    "                        except AttributeError:\n",
    "                            company = \"Not listed\"\n",
    "                        data['Company'] = company\n",
    "\n",
    "                        try:\n",
    "                            location = job.find('div', class_='css-1p0sjhy').text.strip()\n",
    "                        except AttributeError:\n",
    "                            location = \"Not listed\"\n",
    "                        data['Location'] = location\n",
    "\n",
    "                        salary = job.find('div', class_='css-1cvo3fd')\n",
    "                        data['Salary'] = salary.text.strip() if salary else \"Not listed\"\n",
    "\n",
    "                        try:\n",
    "                            detailed_description = job.find('div', class_='summary').text.strip()\n",
    "                        except AttributeError:\n",
    "                            detailed_description = \"Not listed\"\n",
    "                        data['Description'] = detailed_description\n",
    "\n",
    "                    else:\n",
    "                        print(\"Skip job: \", job_id)\n",
    "\n",
    "                except (TimeoutException, NoSuchElementException):\n",
    "                    detailed_description = \"Not listed\"\n",
    "                    data['Description'] = detailed_description\n",
    "\n",
    "                jobs_data.append(data)\n",
    "\n",
    "# Close the WebDriver after finishing\n",
    "driver.quit()\n",
    "\n",
    "# Save data to CSV file\n",
    "csv_file_path = \"fuckthis.csv\"\n",
    "with open(csv_file_path, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "    fieldnames = ['Title', 'Company', 'Location', 'Salary', 'Description', 'Apply_Link']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for job_data in jobs_data:\n",
    "        writer.writerow(job_data)\n",
    "\n",
    "print(\"Data saved to CSV file successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "231c32e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = Driver(uc = True)\n",
    "\n",
    "# Setup explicit wait\n",
    "wait = WebDriverWait(driver, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8d27ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 completed.\n",
      "Page 2 completed.\n",
      "Page 3 completed.\n",
      "Page 4 completed.\n",
      "Page 5 completed.\n",
      "Page 6 completed.\n",
      "Page 7 completed.\n",
      "Page 8 completed.\n",
      "Page 9 completed.\n",
      "Page 10 completed.\n",
      "Page 11 completed.\n",
      "Page 12 completed.\n",
      "Page 13 completed.\n",
      "Page 14 completed.\n",
      "Page 15 completed.\n",
      "Page 16 completed.\n",
      "Page 17 completed.\n",
      "Page 18 completed.\n",
      "Page 19 completed.\n",
      "Page 20 completed.\n",
      "Job extraction complete. Data saved to 'sustainability_intern.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the list to store job data\n",
    "jobs_data = []\n",
    "\n",
    "# Initialize the page variable\n",
    "page = 0\n",
    "\n",
    "# Loop through multiple pages\n",
    "for i in range(0, 200, 10):\n",
    "    driver.get('https://www.indeed.com/jobs?q=green+economy+entry+level%20&l=united%20states&start=' + str(i))\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.job_seen_beacon')))\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "        break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.findAll('div', class_='job_seen_beacon')\n",
    "\n",
    "    if not jobs:\n",
    "        print(\"No more jobs found.\")\n",
    "        break\n",
    "\n",
    "    for job in jobs:\n",
    "        data = {}\n",
    "\n",
    "        try:\n",
    "            data['Title'] = job.find('h2', class_='jobTitle').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Title'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Company'] = job.find('span', class_=\"css-92r8pb\").text.strip()\n",
    "        except AttributeError:\n",
    "            data['Company'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            location = job.find('div', class_='css-1p0sjhy').text.strip()\n",
    "        except AttributeError:\n",
    "            location = \"Not listed\"\n",
    "        data['Location'] = location\n",
    "\n",
    "        skills_section = job.find('div', class_='jobsearch-SkillsWrapper')\n",
    "        data['Skills'] = ', '.join([skill.text.strip() for skill in skills_section.find_all('span', class_='jobsearch-SkillTag')]) if skills_section else \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Salary'] = job.find('div', class_='css-1cvo3fd').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Salary'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            wait.until(EC.presence_of_element_located((By.ID, 'jobDescriptionText')))\n",
    "            detailed_description = driver.find_element(By.ID, 'jobDescriptionText').text.strip()\n",
    "        except (TimeoutException, NoSuchElementException):\n",
    "            detailed_description = \"Not listed\"\n",
    "\n",
    "        data['Description'] = detailed_description\n",
    "\n",
    "        try:\n",
    "            job_link = job.find('a', class_='jcs-JobTitle')['href']\n",
    "            data['Apply Link'] = 'https://www.indeed.com' + job_link\n",
    "        except TypeError:\n",
    "            data['Apply Link'] = \"Not listed\"\n",
    "\n",
    "        jobs_data.append(data)\n",
    "\n",
    "    page += 1\n",
    "    print(f\"Page {page} completed.\")\n",
    "\n",
    "# Quit the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Create DataFrame and save to CSV\n",
    "df = pd.DataFrame(jobs_data)\n",
    "df.to_csv('green economy.csv', index=False)\n",
    "print(\"Job extraction complete. Data saved to 'sustainability_intern.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7aa5856b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 13630 completed.\n",
      "Page 13645 completed.\n",
      "Page 13660 completed.\n",
      "Page 13675 completed.\n",
      "Page 13690 completed.\n",
      "Page 13705 completed.\n",
      "Page 13720 completed.\n",
      "Page 13735 completed.\n",
      "Page 13750 completed.\n",
      "Page 13765 completed.\n",
      "Page 13780 completed.\n",
      "Page 13795 completed.\n",
      "Page 13810 completed.\n",
      "Page 13825 completed.\n",
      "Page 13840 completed.\n",
      "Page 13855 completed.\n",
      "Page 13870 completed.\n",
      "Page 13885 completed.\n",
      "Page 13900 completed.\n",
      "Page 13915 completed.\n",
      "Page 13930 completed.\n",
      "Page 13945 completed.\n",
      "Page 13960 completed.\n",
      "Page 13975 completed.\n",
      "Page 13990 completed.\n",
      "Page 14005 completed.\n",
      "Page 14020 completed.\n",
      "Page 14035 completed.\n",
      "Page 14050 completed.\n",
      "Page 14065 completed.\n",
      "Page 14080 completed.\n",
      "Page 14095 completed.\n",
      "Page 14110 completed.\n",
      "Page 14125 completed.\n",
      "Page 14140 completed.\n",
      "Page 14155 completed.\n",
      "Page 14170 completed.\n",
      "Page 14185 completed.\n",
      "Page 14200 completed.\n",
      "Page 14215 completed.\n",
      "Page 14230 completed.\n",
      "Page 14245 completed.\n",
      "Page 14260 completed.\n",
      "Page 14275 completed.\n",
      "Page 14290 completed.\n",
      "Page 14305 completed.\n",
      "Page 14320 completed.\n",
      "Page 14335 completed.\n",
      "Page 14350 completed.\n",
      "Page 14365 completed.\n",
      "Page 14380 completed.\n",
      "Page 14395 completed.\n",
      "Page 14410 completed.\n",
      "Page 14425 completed.\n",
      "Page 14440 completed.\n",
      "Page 14455 completed.\n",
      "Page 14470 completed.\n",
      "Page 14485 completed.\n",
      "Page 14500 completed.\n",
      "Page 14515 completed.\n",
      "Page 14530 completed.\n",
      "Page 14545 completed.\n",
      "Page 14560 completed.\n",
      "Page 14575 completed.\n",
      "Page 14590 completed.\n",
      "Page 14605 completed.\n",
      "Page 14620 completed.\n",
      "Page 14635 completed.\n",
      "Page 14650 completed.\n",
      "Page 14665 completed.\n",
      "Page 14680 completed.\n",
      "Page 14695 completed.\n",
      "Page 14710 completed.\n",
      "Page 14725 completed.\n",
      "Page 14740 completed.\n",
      "Page 14755 completed.\n",
      "Page 14770 completed.\n",
      "Page 14785 completed.\n",
      "Page 14800 completed.\n",
      "Page 14815 completed.\n",
      "Page 14830 completed.\n",
      "Page 14845 completed.\n",
      "Page 14860 completed.\n",
      "Page 14875 completed.\n",
      "Page 14890 completed.\n",
      "Page 14905 completed.\n",
      "Page 14920 completed.\n",
      "Page 14935 completed.\n",
      "Page 14950 completed.\n",
      "Page 14965 completed.\n",
      "Page 14980 completed.\n",
      "Page 14995 completed.\n",
      "Page 15010 completed.\n",
      "Page 15025 completed.\n",
      "Page 15040 completed.\n",
      "Page 15055 completed.\n",
      "Page 15070 completed.\n",
      "Page 15085 completed.\n",
      "Page 15100 completed.\n",
      "Page 15115 completed.\n",
      "Page 15130 completed.\n",
      "Page 15145 completed.\n",
      "Page 15160 completed.\n",
      "Page 15175 completed.\n",
      "Page 15190 completed.\n",
      "Page 15205 completed.\n",
      "Page 15220 completed.\n",
      "Page 15235 completed.\n",
      "Page 15250 completed.\n",
      "Page 15265 completed.\n",
      "Page 15280 completed.\n",
      "Page 15295 completed.\n",
      "Page 15310 completed.\n",
      "Page 15325 completed.\n",
      "Page 15340 completed.\n",
      "Page 15355 completed.\n",
      "Page 15370 completed.\n",
      "Page 15385 completed.\n",
      "Page 15400 completed.\n",
      "Page 15415 completed.\n",
      "Page 15430 completed.\n",
      "Page 15445 completed.\n",
      "Page 15460 completed.\n",
      "Page 15475 completed.\n",
      "Page 15490 completed.\n",
      "Page 15505 completed.\n",
      "Page 15520 completed.\n",
      "Page 15535 completed.\n",
      "Page 15550 completed.\n",
      "Page 15565 completed.\n",
      "Page 15580 completed.\n",
      "Page 15595 completed.\n",
      "Page 15610 completed.\n",
      "Page 15625 completed.\n",
      "Page 15640 completed.\n",
      "Page 15655 completed.\n",
      "Page 15670 completed.\n",
      "Page 15685 completed.\n",
      "Page 15700 completed.\n",
      "Page 15715 completed.\n",
      "Page 15730 completed.\n",
      "Page 15745 completed.\n",
      "Page 15760 completed.\n",
      "Page 15775 completed.\n",
      "Page 15790 completed.\n",
      "Page 15805 completed.\n",
      "Page 15820 completed.\n",
      "Page 15835 completed.\n",
      "Page 15850 completed.\n",
      "Page 15865 completed.\n",
      "Page 15880 completed.\n",
      "Page 15895 completed.\n",
      "Page 15910 completed.\n",
      "Page 15925 completed.\n",
      "Page 15940 completed.\n",
      "Page 15955 completed.\n",
      "Page 15970 completed.\n",
      "Page 15985 completed.\n",
      "Page 16000 completed.\n",
      "Page 16015 completed.\n",
      "Page 16030 completed.\n",
      "Page 16045 completed.\n",
      "Page 16060 completed.\n",
      "Page 16075 completed.\n",
      "Page 16090 completed.\n",
      "Page 16105 completed.\n",
      "Page 16120 completed.\n",
      "Page 16135 completed.\n",
      "Page 16150 completed.\n",
      "Page 16165 completed.\n",
      "Page 16180 completed.\n",
      "Page 16195 completed.\n",
      "Page 16210 completed.\n",
      "Page 16225 completed.\n",
      "Page 16240 completed.\n",
      "Page 16255 completed.\n",
      "Page 16270 completed.\n",
      "Page 16285 completed.\n",
      "Page 16300 completed.\n",
      "Page 16315 completed.\n",
      "Page 16330 completed.\n",
      "Page 16345 completed.\n",
      "Page 16360 completed.\n",
      "Page 16375 completed.\n",
      "Page 16390 completed.\n",
      "Page 16405 completed.\n",
      "Page 16420 completed.\n",
      "Page 16435 completed.\n",
      "Page 16450 completed.\n",
      "Page 16465 completed.\n",
      "Page 16480 completed.\n",
      "Page 16495 completed.\n",
      "Page 16510 completed.\n",
      "Page 16525 completed.\n",
      "Page 16540 completed.\n",
      "Page 16555 completed.\n",
      "Page 16570 completed.\n",
      "Page 16585 completed.\n",
      "Page 16600 completed.\n",
      "Page 16615 completed.\n",
      "Job extraction complete. Data saved to 'indeed_jobs.csv'.\n"
     ]
    }
   ],
   "source": [
    "jobs_data = []\n",
    "#https://www.indeed.com/jobs?q=sustainability+entry+level&l=united+states&vjk=e7d490779f2eb29f\n",
    "\n",
    "for i in range(0,2000,10):\n",
    "    driver.get('https://www.indeed.com/jobs?q=sustainability+entry+level%20&l=united%20states&start='+str(i))\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    #https://www.indeed.com/jobs?q=sustainability++jobs&l=USA&vjk=0cdb55029eb47b25\n",
    "\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.job_seen_beacon')))\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "        break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.findAll('div', class_='job_seen_beacon')\n",
    "\n",
    "    if not jobs:\n",
    "        print(\"No more jobs found.\")\n",
    "        break\n",
    "\n",
    "    for job in jobs:\n",
    "        data = {}\n",
    "\n",
    "\n",
    "        try:\n",
    "          wait.until(EC.presence_of_element_located((By.ID, 'jobDescriptionText')))\n",
    "          detailed_description = driver.find_element(By.ID, 'jobDescriptionText').text.strip()\n",
    "        except (TimeoutException, NoSuchElementException):\n",
    "          detailed_description = \"Not listed\"\n",
    "\n",
    "          data['Description'] = detailed_description\n",
    "\n",
    "        try:\n",
    "            data['Title'] = job.find('h2', class_='jobTitle').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Title'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Company'] = job.find('span', class_=\"css-92r8pb\").text.strip()\n",
    "        except AttributeError:\n",
    "            data['Company'] = \"Not listed\"\n",
    "    \n",
    "\n",
    "        try:\n",
    "          \n",
    "            location = job.find('div', class_='css-1p0sjhy').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Location'] = \"Not listed\"\n",
    "        data['Location'] = location\n",
    "\n",
    "        '''try:\n",
    "            data['Location'] = job.find('div', class_='company_location').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Location'] = \"Not listed\"\n",
    "'''\n",
    "\n",
    "        skills_section = job.find('div', class_='jobsearch-SkillsWrapper')\n",
    "        data['Skills'] = ', '.join([skill.text.strip() for skill in skills_section.find_all('span', class_='jobsearch-SkillTag')]) if skills_section else \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Salary'] = job.find('div', class_='css-1cvo3fd').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Salary'] = \"Not listed\"\n",
    "\n",
    "\n",
    "        try:\n",
    "            wait.until(EC.presence_of_element_located((By.ID, 'jobDescriptionText')))\n",
    "            detailed_description = driver.find_element(By.ID, 'jobDescriptionText').text.strip()\n",
    "        except (TimeoutException, NoSuchElementException):\n",
    "            detailed_description = \"Not listed\"\n",
    "\n",
    "        data['Description'] = detailed_description\n",
    "\n",
    "\n",
    "        try:\n",
    "            job_link = job.find('a', class_='jcs-JobTitle')['href']\n",
    "            data['Apply Link'] = 'https://www.indeed.com' + job_link\n",
    "        except TypeError:\n",
    "            data['Apply Link'] = \"Not listed\"\n",
    "\n",
    "        jobs_data.append(data)\n",
    "        page += 1\n",
    "    print(f\"Page {page + 1} completed.\")\n",
    "\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df = pd.DataFrame(jobs_data)\n",
    "df.to_csv('entry_level_scrapping.csv', index=False)\n",
    "print(\"Job extraction complete. Data saved to 'indeed_jobs.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b3b08890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 17140 completed.\n",
      "Page 17155 completed.\n",
      "Page 17170 completed.\n",
      "Page 17185 completed.\n",
      "Page 17200 completed.\n",
      "Page 17215 completed.\n",
      "Page 17230 completed.\n",
      "Page 17245 completed.\n",
      "Page 17260 completed.\n",
      "Page 17275 completed.\n",
      "Page 17290 completed.\n",
      "Page 17305 completed.\n",
      "Page 17320 completed.\n",
      "Page 17335 completed.\n",
      "Page 17350 completed.\n",
      "Page 17365 completed.\n",
      "Page 17380 completed.\n",
      "Page 17395 completed.\n",
      "Page 17410 completed.\n",
      "Page 17425 completed.\n",
      "Page 17440 completed.\n",
      "Page 17455 completed.\n",
      "Page 17470 completed.\n",
      "Page 17485 completed.\n",
      "Page 17500 completed.\n",
      "Page 17515 completed.\n",
      "Page 17530 completed.\n",
      "Page 17545 completed.\n",
      "Page 17560 completed.\n",
      "Page 17575 completed.\n",
      "Page 17590 completed.\n",
      "Page 17605 completed.\n",
      "Page 17620 completed.\n",
      "Page 17635 completed.\n",
      "Page 17650 completed.\n",
      "Page 17665 completed.\n",
      "Page 17680 completed.\n",
      "Page 17695 completed.\n",
      "Page 17710 completed.\n",
      "Page 17725 completed.\n",
      "Page 17740 completed.\n",
      "Page 17755 completed.\n",
      "Page 17770 completed.\n",
      "Page 17785 completed.\n",
      "Page 17800 completed.\n",
      "Page 17815 completed.\n",
      "Page 17830 completed.\n",
      "Page 17845 completed.\n",
      "Page 17860 completed.\n",
      "Page 17875 completed.\n",
      "Page 17890 completed.\n",
      "Page 17905 completed.\n",
      "Page 17920 completed.\n",
      "Page 17935 completed.\n",
      "Page 17950 completed.\n",
      "Page 17965 completed.\n",
      "Page 17980 completed.\n",
      "Page 17995 completed.\n",
      "Page 18010 completed.\n",
      "Page 18025 completed.\n",
      "Page 18040 completed.\n",
      "Page 18055 completed.\n",
      "Page 18070 completed.\n",
      "Page 18078 completed.\n",
      "Page 18093 completed.\n",
      "Page 18108 completed.\n",
      "Page 18123 completed.\n",
      "Page 18138 completed.\n",
      "Page 18153 completed.\n",
      "Page 18168 completed.\n",
      "Page 18183 completed.\n",
      "Page 18198 completed.\n",
      "Page 18213 completed.\n",
      "Page 18228 completed.\n",
      "Page 18243 completed.\n",
      "Page 18258 completed.\n",
      "Page 18273 completed.\n",
      "Page 18288 completed.\n",
      "Page 18303 completed.\n",
      "Page 18318 completed.\n",
      "Page 18333 completed.\n",
      "Page 18348 completed.\n",
      "Page 18363 completed.\n",
      "Page 18378 completed.\n",
      "Page 18393 completed.\n",
      "Page 18408 completed.\n",
      "Page 18423 completed.\n",
      "Page 18438 completed.\n",
      "Page 18453 completed.\n",
      "Page 18468 completed.\n",
      "Page 18483 completed.\n",
      "Page 18498 completed.\n",
      "Page 18513 completed.\n",
      "Page 18528 completed.\n",
      "Page 18543 completed.\n",
      "Page 18558 completed.\n",
      "Page 18573 completed.\n",
      "Page 18588 completed.\n",
      "Page 18603 completed.\n",
      "Page 18618 completed.\n",
      "Page 18633 completed.\n",
      "Page 18648 completed.\n",
      "Page 18663 completed.\n",
      "Page 18678 completed.\n",
      "Page 18693 completed.\n",
      "Page 18708 completed.\n",
      "Page 18723 completed.\n",
      "Page 18738 completed.\n",
      "Page 18753 completed.\n",
      "Page 18768 completed.\n",
      "Page 18783 completed.\n",
      "Page 18798 completed.\n",
      "Page 18813 completed.\n",
      "Page 18828 completed.\n",
      "Page 18843 completed.\n",
      "Page 18858 completed.\n",
      "Page 18873 completed.\n",
      "Page 18888 completed.\n",
      "Page 18903 completed.\n",
      "Page 18918 completed.\n",
      "Page 18933 completed.\n",
      "Page 18948 completed.\n",
      "Page 18963 completed.\n",
      "Page 18978 completed.\n",
      "Page 18993 completed.\n",
      "Page 19008 completed.\n",
      "Page 19023 completed.\n",
      "Page 19038 completed.\n",
      "Page 19053 completed.\n",
      "Page 19068 completed.\n",
      "Page 19083 completed.\n",
      "Page 19098 completed.\n",
      "Page 19113 completed.\n",
      "Page 19128 completed.\n",
      "Page 19143 completed.\n",
      "Page 19158 completed.\n",
      "Page 19173 completed.\n",
      "Page 19188 completed.\n",
      "Page 19203 completed.\n",
      "Page 19218 completed.\n",
      "Page 19233 completed.\n",
      "Page 19248 completed.\n",
      "Page 19263 completed.\n",
      "Page 19278 completed.\n",
      "Page 19293 completed.\n",
      "Page 19308 completed.\n",
      "Page 19323 completed.\n",
      "Page 19338 completed.\n",
      "Page 19353 completed.\n",
      "Page 19368 completed.\n",
      "Page 19383 completed.\n",
      "Page 19398 completed.\n",
      "Page 19413 completed.\n",
      "Page 19428 completed.\n",
      "Page 19443 completed.\n",
      "Page 19458 completed.\n",
      "Page 19473 completed.\n",
      "Page 19488 completed.\n",
      "Page 19503 completed.\n",
      "Page 19518 completed.\n",
      "Page 19533 completed.\n",
      "Page 19548 completed.\n",
      "Page 19563 completed.\n",
      "Page 19578 completed.\n",
      "Page 19593 completed.\n",
      "Page 19608 completed.\n",
      "Page 19623 completed.\n",
      "Page 19638 completed.\n",
      "Page 19653 completed.\n",
      "Page 19668 completed.\n",
      "Page 19683 completed.\n",
      "Page 19698 completed.\n",
      "Page 19713 completed.\n",
      "Page 19728 completed.\n",
      "Page 19743 completed.\n",
      "Page 19758 completed.\n",
      "Page 19773 completed.\n",
      "Page 19788 completed.\n",
      "Page 19803 completed.\n",
      "Page 19818 completed.\n",
      "Page 19833 completed.\n",
      "Page 19848 completed.\n",
      "Page 19863 completed.\n",
      "Page 19878 completed.\n",
      "Page 19893 completed.\n",
      "Page 19908 completed.\n",
      "Page 19923 completed.\n",
      "Page 19938 completed.\n",
      "Page 19953 completed.\n",
      "Page 19968 completed.\n",
      "Page 19983 completed.\n",
      "Page 19998 completed.\n",
      "Page 20013 completed.\n",
      "Page 20028 completed.\n",
      "Page 20043 completed.\n",
      "Page 20058 completed.\n",
      "Page 20073 completed.\n",
      "Page 20088 completed.\n",
      "Page 20103 completed.\n",
      "Page 20118 completed.\n",
      "Job extraction complete. Data saved to 'indeed_jobs.csv'.\n"
     ]
    }
   ],
   "source": [
    "jobs_data = []\n",
    "#https://www.indeed.com/jobs?q=sustainability+entry+level&l=united+states&vjk=e7d490779f2eb29f\n",
    "\n",
    "for i in range(0,2000,10):\n",
    "    driver.get('https://www.indeed.com/jobs?q=sustainability+entry+level%20&l=united%20states&start='+str(i))\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    #https://www.indeed.com/jobs?q=sustainability++jobs&l=USA&vjk=0cdb55029eb47b25\n",
    "\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.job_seen_beacon')))\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "        break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.findAll('div', class_='job_seen_beacon')\n",
    "\n",
    "    if not jobs:\n",
    "        print(\"No more jobs found.\")\n",
    "        break\n",
    "\n",
    "    for job in jobs:\n",
    "        data = {}\n",
    "\n",
    "\n",
    "        try:\n",
    "          wait.until(EC.presence_of_element_located((By.ID, 'jobDescriptionText')))\n",
    "          detailed_description = driver.find_element(By.ID, 'jobDescriptionText').text.strip()\n",
    "        except (TimeoutException, NoSuchElementException):\n",
    "          detailed_description = \"Not listed\"\n",
    "\n",
    "          data['Description'] = detailed_description\n",
    "\n",
    "        try:\n",
    "            data['Title'] = job.find('h2', class_='jobTitle').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Title'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Company'] = job.find('span', class_=\"css-92r8pb\").text.strip()\n",
    "        except AttributeError:\n",
    "            data['Company'] = \"Not listed\"\n",
    "    \n",
    "\n",
    "        try:\n",
    "          \n",
    "            location = job.find('div', class_='css-1p0sjhy').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Location'] = \"Not listed\"\n",
    "        data['Location'] = location\n",
    "\n",
    "        '''try:\n",
    "            data['Location'] = job.find('div', class_='company_location').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Location'] = \"Not listed\"\n",
    "'''\n",
    "\n",
    "        skills_section = job.find('div', class_='jobsearch-SkillsWrapper')\n",
    "        data['Skills'] = ', '.join([skill.text.strip() for skill in skills_section.find_all('span', class_='jobsearch-SkillTag')]) if skills_section else \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Salary'] = job.find('div', class_='css-1cvo3fd').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Salary'] = \"Not listed\"\n",
    "\n",
    "\n",
    "        try:\n",
    "            wait.until(EC.presence_of_element_located((By.ID, 'jobDescriptionText')))\n",
    "            detailed_description = driver.find_element(By.ID, 'jobDescriptionText').text.strip()\n",
    "        except (TimeoutException, NoSuchElementException):\n",
    "            detailed_description = \"Not listed\"\n",
    "\n",
    "        data['Description'] = detailed_description\n",
    "\n",
    "\n",
    "        try:\n",
    "            job_link = job.find('a', class_='jcs-JobTitle')['href']\n",
    "            data['Apply Link'] = 'https://www.indeed.com' + job_link\n",
    "        except TypeError:\n",
    "            data['Apply Link'] = \"Not listed\"\n",
    "\n",
    "        jobs_data.append(data)\n",
    "        page += 1\n",
    "    print(f\"Page {page + 1} completed.\")\n",
    "\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df = pd.DataFrame(jobs_data)\n",
    "df.to_csv('entry_level_scrapping_second.csv', index=False)\n",
    "print(\"Job extraction complete. Data saved to 'indeed_jobs.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78496e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f07dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221746ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3413a201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 10355 completed.\n",
      "Page 10370 completed.\n",
      "Page 10385 completed.\n",
      "Page 10400 completed.\n",
      "Page 10415 completed.\n",
      "Page 10430 completed.\n",
      "Page 10445 completed.\n",
      "Page 10460 completed.\n",
      "Page 10475 completed.\n",
      "Page 10490 completed.\n",
      "Page 10505 completed.\n",
      "Page 10520 completed.\n",
      "Page 10535 completed.\n",
      "Page 10550 completed.\n",
      "Page 10565 completed.\n",
      "Page 10580 completed.\n",
      "Page 10595 completed.\n",
      "Page 10610 completed.\n",
      "Page 10625 completed.\n",
      "Page 10640 completed.\n",
      "Page 10655 completed.\n",
      "Page 10670 completed.\n",
      "Page 10685 completed.\n",
      "Page 10700 completed.\n",
      "Page 10715 completed.\n",
      "Page 10730 completed.\n",
      "Page 10745 completed.\n",
      "Page 10760 completed.\n",
      "Page 10775 completed.\n",
      "Page 10790 completed.\n",
      "Page 10805 completed.\n",
      "Page 10820 completed.\n",
      "Page 10835 completed.\n",
      "Page 10850 completed.\n",
      "Page 10865 completed.\n",
      "Page 10880 completed.\n",
      "Page 10895 completed.\n",
      "Page 10910 completed.\n",
      "Page 10925 completed.\n",
      "Page 10940 completed.\n",
      "Page 10955 completed.\n",
      "Page 10970 completed.\n",
      "Page 10985 completed.\n",
      "Page 11000 completed.\n",
      "Page 11015 completed.\n",
      "Page 11030 completed.\n",
      "Page 11045 completed.\n",
      "Page 11060 completed.\n",
      "Page 11075 completed.\n",
      "Page 11090 completed.\n",
      "Page 11105 completed.\n",
      "Page 11120 completed.\n",
      "Page 11135 completed.\n",
      "Page 11150 completed.\n",
      "Page 11165 completed.\n",
      "Page 11180 completed.\n",
      "Page 11195 completed.\n",
      "Page 11210 completed.\n",
      "Page 11225 completed.\n",
      "Page 11240 completed.\n",
      "Page 11255 completed.\n",
      "Page 11270 completed.\n",
      "Page 11285 completed.\n",
      "Page 11300 completed.\n",
      "Page 11315 completed.\n",
      "Page 11320 completed.\n",
      "Page 11335 completed.\n",
      "Page 11350 completed.\n",
      "Page 11365 completed.\n",
      "Page 11380 completed.\n",
      "Page 11395 completed.\n",
      "Page 11410 completed.\n",
      "Page 11425 completed.\n",
      "Page 11440 completed.\n",
      "Page 11455 completed.\n",
      "Page 11470 completed.\n",
      "Page 11485 completed.\n",
      "Page 11500 completed.\n",
      "Page 11515 completed.\n",
      "Page 11530 completed.\n",
      "Page 11545 completed.\n",
      "Page 11560 completed.\n",
      "Page 11575 completed.\n",
      "Page 11590 completed.\n",
      "Page 11605 completed.\n",
      "Page 11620 completed.\n",
      "Page 11635 completed.\n",
      "Page 11650 completed.\n",
      "Page 11665 completed.\n",
      "Page 11680 completed.\n",
      "Page 11695 completed.\n",
      "Page 11710 completed.\n",
      "Page 11725 completed.\n",
      "Page 11740 completed.\n",
      "Page 11755 completed.\n",
      "Page 11770 completed.\n",
      "Page 11785 completed.\n",
      "Page 11800 completed.\n",
      "Page 11815 completed.\n",
      "Page 11830 completed.\n",
      "Page 11845 completed.\n",
      "Page 11860 completed.\n",
      "Page 11875 completed.\n",
      "Page 11890 completed.\n",
      "Page 11905 completed.\n",
      "Page 11920 completed.\n",
      "Page 11935 completed.\n",
      "Page 11950 completed.\n",
      "Page 11965 completed.\n",
      "Page 11980 completed.\n",
      "Page 11995 completed.\n",
      "Page 12010 completed.\n",
      "Page 12025 completed.\n",
      "Page 12040 completed.\n",
      "Page 12055 completed.\n",
      "Page 12070 completed.\n",
      "Page 12085 completed.\n",
      "Page 12100 completed.\n",
      "Page 12115 completed.\n",
      "Page 12130 completed.\n",
      "Page 12145 completed.\n",
      "Page 12160 completed.\n",
      "Page 12175 completed.\n",
      "Page 12190 completed.\n",
      "Page 12205 completed.\n",
      "Page 12220 completed.\n",
      "Page 12235 completed.\n",
      "Page 12250 completed.\n",
      "Page 12265 completed.\n",
      "Page 12280 completed.\n",
      "Page 12295 completed.\n",
      "Page 12310 completed.\n",
      "Page 12325 completed.\n",
      "Page 12340 completed.\n",
      "Page 12355 completed.\n",
      "Page 12370 completed.\n",
      "Page 12385 completed.\n",
      "Page 12400 completed.\n",
      "Page 12415 completed.\n",
      "Page 12430 completed.\n",
      "Page 12445 completed.\n",
      "Page 12460 completed.\n",
      "Page 12475 completed.\n",
      "Page 12490 completed.\n",
      "Page 12505 completed.\n",
      "Page 12520 completed.\n",
      "Page 12535 completed.\n",
      "Page 12550 completed.\n",
      "Page 12565 completed.\n",
      "Page 12580 completed.\n",
      "Page 12595 completed.\n",
      "Page 12610 completed.\n",
      "Page 12625 completed.\n",
      "Page 12640 completed.\n",
      "Page 12655 completed.\n",
      "Page 12670 completed.\n",
      "Page 12685 completed.\n",
      "Page 12700 completed.\n",
      "Page 12715 completed.\n",
      "Page 12730 completed.\n",
      "Page 12745 completed.\n",
      "Page 12760 completed.\n",
      "Page 12775 completed.\n",
      "Page 12790 completed.\n",
      "Page 12805 completed.\n",
      "Page 12820 completed.\n",
      "Page 12835 completed.\n",
      "Page 12850 completed.\n",
      "Page 12865 completed.\n",
      "Page 12880 completed.\n",
      "Page 12895 completed.\n",
      "Page 12910 completed.\n",
      "Page 12925 completed.\n",
      "Page 12940 completed.\n",
      "Page 12955 completed.\n",
      "Page 12970 completed.\n",
      "Page 12985 completed.\n",
      "Page 13000 completed.\n",
      "Page 13015 completed.\n",
      "Page 13030 completed.\n",
      "Page 13045 completed.\n",
      "Page 13060 completed.\n",
      "Page 13075 completed.\n",
      "Page 13090 completed.\n",
      "Page 13105 completed.\n",
      "Page 13120 completed.\n",
      "Page 13135 completed.\n",
      "Page 13150 completed.\n",
      "Page 13165 completed.\n",
      "Page 13180 completed.\n",
      "Page 13195 completed.\n",
      "Page 13210 completed.\n",
      "Page 13225 completed.\n",
      "Page 13240 completed.\n",
      "Page 13255 completed.\n",
      "Page 13270 completed.\n",
      "Page 13285 completed.\n",
      "Page 13300 completed.\n",
      "Page 13315 completed.\n",
      "Page 13330 completed.\n",
      "Job extraction complete. Data saved to 'indeed_jobs.csv'.\n"
     ]
    }
   ],
   "source": [
    "jobs_data = []\n",
    "\n",
    "\n",
    "for i in range(0,2000,10):\n",
    "    driver.get('https://www.indeed.com/jobs?q=sustainability%20&l=united%20states&start='+str(i))\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    #https://www.indeed.com/jobs?q=sustainability++jobs&l=USA&vjk=0cdb55029eb47b25\n",
    "\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.job_seen_beacon')))\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "        break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.findAll('div', class_='job_seen_beacon')\n",
    "\n",
    "    if not jobs:\n",
    "        print(\"No more jobs found.\")\n",
    "        break\n",
    "\n",
    "    for job in jobs:\n",
    "        data = {}\n",
    "\n",
    "\n",
    "        try:\n",
    "          wait.until(EC.presence_of_element_located((By.ID, 'jobDescriptionText')))\n",
    "          detailed_description = driver.find_element(By.ID, 'jobDescriptionText').text.strip()\n",
    "        except (TimeoutException, NoSuchElementException):\n",
    "          detailed_description = \"Not listed\"\n",
    "          \n",
    "          data['Description'] = detailed_description\n",
    "\n",
    "        try:\n",
    "            data['Title'] = job.find('h2', class_='jobTitle').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Title'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Company'] = job.find('span', class_=\"css-92r8pb\").text.strip()\n",
    "        except AttributeError:\n",
    "            data['Company'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Location'] = job.find('div', class_='company_location').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Location'] = \"Not listed\"\n",
    "\n",
    "        skills_section = job.find('div', class_='jobsearch-SkillsWrapper')\n",
    "        data['Skills'] = ', '.join([skill.text.strip() for skill in skills_section.find_all('span', class_='jobsearch-SkillTag')]) if skills_section else \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Salary'] = job.find('div', class_='css-1cvo3fd').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Salary'] = \"Not listed\"\n",
    "\n",
    "\n",
    "        try:\n",
    "            wait.until(EC.presence_of_element_located((By.ID, 'jobDescriptionText')))\n",
    "            detailed_description = driver.find_element(By.ID, 'jobDescriptionText').text.strip()\n",
    "        except (TimeoutException, NoSuchElementException):\n",
    "            detailed_description = \"Not listed\"\n",
    "\n",
    "        data['Description'] = detailed_description\n",
    "\n",
    "        \n",
    "        try:\n",
    "            job_link = job.find('a', class_='jcs-JobTitle')['href']\n",
    "            data['Apply Link'] = 'https://www.indeed.com' + job_link\n",
    "        except TypeError:\n",
    "            data['Apply Link'] = \"Not listed\"\n",
    "\n",
    "        jobs_data.append(data)\n",
    "        page += 1\n",
    "    print(f\"Page {page + 1} completed.\")\n",
    "\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df = pd.DataFrame(jobs_data)\n",
    "df.to_csv('clean_scrapped.csv', index=False)\n",
    "print(\"Job extraction complete. Data saved to 'indeed_jobs.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "618f5426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 3550 completed.\n",
      "Page 3565 completed.\n",
      "Page 3580 completed.\n",
      "Page 3595 completed.\n",
      "Page 3610 completed.\n",
      "Job extraction complete. Data saved to 'indeed_jobs.csv'.\n"
     ]
    }
   ],
   "source": [
    "jobs_data = []\n",
    "\n",
    "\n",
    "for i in range(0,50,10):\n",
    "    driver.get('https://www.indeed.com/jobs?q=sustainability%20&l=united%20states&start='+str(i))\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "\n",
    "    #https://www.indeed.com/jobs?q=sustainability++jobs&l=USA&vjk=0cdb55029eb47b25\n",
    "\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.job_seen_beacon')))\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "        break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.findAll('div', class_='job_seen_beacon')\n",
    "\n",
    "    if not jobs:\n",
    "        print(\"No more jobs found.\")\n",
    "        break\n",
    "\n",
    "    for job in jobs:\n",
    "        data = {}\n",
    "\n",
    "\n",
    "        try:\n",
    "          wait.until(EC.presence_of_element_located((By.ID, 'jobDescriptionText')))\n",
    "          detailed_description = driver.find_element(By.ID, 'jobDescriptionText').text.strip()\n",
    "        except (TimeoutException, NoSuchElementException):\n",
    "          detailed_description = \"Not listed\"\n",
    "          \n",
    "          data['Description'] = detailed_description\n",
    "\n",
    "        try:\n",
    "            data['Title'] = job.find('h2', class_='jobTitle').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Title'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Company'] = job.find('span', class_=\"css-92r8pb\").text.strip()\n",
    "        except AttributeError:\n",
    "            data['Company'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Location'] = job.find('div', class_='company_location').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Location'] = \"Not listed\"\n",
    "\n",
    "        skills_section = job.find('div', class_='jobsearch-SkillsWrapper')\n",
    "        data['Skills'] = ', '.join([skill.text.strip() for skill in skills_section.find_all('span', class_='jobsearch-SkillTag')]) if skills_section else \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Salary'] = job.find('div', class_='css-1cvo3fd').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Salary'] = \"Not listed\"\n",
    "\n",
    "\n",
    "        try:\n",
    "            wait.until(EC.presence_of_element_located((By.ID, 'jobDescriptionText')))\n",
    "            detailed_description = driver.find_element(By.ID, 'jobDescriptionText').text.strip()\n",
    "        except (TimeoutException, NoSuchElementException):\n",
    "            detailed_description = \"Not listed\"\n",
    "\n",
    "        data['Description'] = detailed_description\n",
    "\n",
    "        \n",
    "        try:\n",
    "            job_link = job.find('a', class_='jcs-JobTitle')['href']\n",
    "            data['Apply Link'] = 'https://www.indeed.com' + job_link\n",
    "        except TypeError:\n",
    "            data['Apply Link'] = \"Not listed\"\n",
    "\n",
    "        jobs_data.append(data)\n",
    "        page += 1\n",
    "    print(f\"Page {page + 1} completed.\")\n",
    "\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df = pd.DataFrame(jobs_data)\n",
    "df.to_csv('tryout.csv', index=False)\n",
    "print(\"Job extraction complete. Data saved to 'indeed_jobs.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f91d0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd5fbe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "21f8c0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 3475 completed.\n",
      "Page 3490 completed.\n",
      "Page 3505 completed.\n",
      "Page 3520 completed.\n",
      "Page 3535 completed.\n",
      "Job extraction complete. Data saved to 'indeed_jobs.csv'.\n"
     ]
    }
   ],
   "source": [
    "jobs_data = []\n",
    "\n",
    "\n",
    "for i in range(0,50,10):\n",
    "    driver.get('https://www.indeed.com/jobs?q=sustainability%20&l=united%20states&start='+str(i))\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "\n",
    "    #https://www.indeed.com/jobs?q=sustainability++jobs&l=USA&vjk=0cdb55029eb47b25\n",
    "\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.job_seen_beacon')))\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "        break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.findAll('div', class_='job_seen_beacon')\n",
    "\n",
    "    if not jobs:\n",
    "        print(\"No more jobs found.\")\n",
    "        break\n",
    "\n",
    "    for job in jobs:\n",
    "        data = {}\n",
    "\n",
    "\n",
    "        try:\n",
    "          wait.until(EC.presence_of_element_located((By.ID, 'jobDescriptionText')))\n",
    "          detailed_description = driver.find_element(By.ID, 'jobDescriptionText').text.strip()\n",
    "        except (TimeoutException, NoSuchElementException):\n",
    "          detailed_description = \"Not listed\"\n",
    "          \n",
    "          data['Description'] = detailed_description\n",
    "\n",
    "        try:\n",
    "            data['Title'] = job.find('h2', class_='jobTitle').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Title'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Company'] = job.find('span', class_=\"css-92r8pb\").text.strip()\n",
    "        except AttributeError:\n",
    "            data['Company'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Location'] = job.find('div', class_='company_location').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Location'] = \"Not listed\"\n",
    "\n",
    "        skills_section = job.find('div', class_='jobsearch-SkillsWrapper')\n",
    "        data['Skills'] = ', '.join([skill.text.strip() for skill in skills_section.find_all('span', class_='jobsearch-SkillTag')]) if skills_section else \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Salary'] = job.find('div', class_='css-1cvo3fd').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Salary'] = \"Not listed\"\n",
    "\n",
    "\n",
    "        try:\n",
    "            jobDescriptionText = job.find('div', class_='jobsearch-jobDescriptionText')\n",
    "            data['Description'] = jobDescriptionText.text.strip() if jobDescriptionText else \"Not listed\"\n",
    "        except AttributeError:\n",
    "            data['Description'] = \"Not listed\"\n",
    "\n",
    "        \n",
    "        try:\n",
    "            job_link = job.find('a', class_='jcs-JobTitle')['href']\n",
    "            data['Apply Link'] = 'https://www.indeed.com' + job_link\n",
    "        except TypeError:\n",
    "            data['Apply Link'] = \"Not listed\"\n",
    "\n",
    "        jobs_data.append(data)\n",
    "        page += 1\n",
    "    print(f\"Page {page + 1} completed.\")\n",
    "\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df = pd.DataFrame(jobs_data)\n",
    "df.to_csv('tryout.csv', index=False)\n",
    "print(\"Job extraction complete. Data saved to 'indeed_jobs.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a405797b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42e39c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd1d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "daafb49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 3400 completed.\n",
      "Page 3415 completed.\n",
      "Page 3430 completed.\n",
      "Page 3445 completed.\n",
      "Page 3460 completed.\n",
      "Job extraction complete. Data saved to 'indeed_jobs.csv'.\n"
     ]
    }
   ],
   "source": [
    "jobs_data = []\n",
    "\n",
    "\n",
    "for i in range(0,50,10):\n",
    "    driver.get('https://www.indeed.com/jobs?q=sustainability%20&l=united%20states&start='+str(i))\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "\n",
    "    #https://www.indeed.com/jobs?q=sustainability++jobs&l=USA&vjk=0cdb55029eb47b25\n",
    "\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.job_seen_beacon')))\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "        break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.findAll('div', class_='job_seen_beacon')\n",
    "\n",
    "    if not jobs:\n",
    "        print(\"No more jobs found.\")\n",
    "        break\n",
    "\n",
    "    for job in jobs:\n",
    "        data = {}\n",
    "\n",
    "\n",
    "        try:\n",
    "          wait.until(EC.presence_of_element_located((By.ID, 'jobDescriptionText')))\n",
    "          detailed_description = driver.find_element(By.ID, 'jobDescriptionText').text.strip()\n",
    "        except (TimeoutException, NoSuchElementException):\n",
    "          detailed_description = \"Not listed\"\n",
    "          \n",
    "          data['Description'] = detailed_description\n",
    "\n",
    "        try:\n",
    "            data['Title'] = job.find('h2', class_='jobTitle').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Title'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Company'] = job.find('span', class_=\"css-92r8pb\").text.strip()\n",
    "        except AttributeError:\n",
    "            data['Company'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Location'] = job.find('div', class_='company_location').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Location'] = \"Not listed\"\n",
    "\n",
    "        skills_section = job.find('div', class_='jobsearch-SkillsWrapper')\n",
    "        data['Skills'] = ', '.join([skill.text.strip() for skill in skills_section.find_all('span', class_='jobsearch-SkillTag')]) if skills_section else \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Salary'] = job.find('div', class_='css-1cvo3fd').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Salary'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            jobDescriptionText = job.find('div', class_='summary').text.strip()\n",
    "        except AttributeError:\n",
    "            detailed_description = \"Not listed\"\n",
    "        data['Description'] =jobDescriptionText\n",
    "        \n",
    "        try:\n",
    "            job_link = job.find('a', class_='jcs-JobTitle')['href']\n",
    "            data['Apply Link'] = 'https://www.indeed.com' + job_link\n",
    "        except TypeError:\n",
    "            data['Apply Link'] = \"Not listed\"\n",
    "\n",
    "        jobs_data.append(data)\n",
    "        page += 1\n",
    "    print(f\"Page {page + 1} completed.\")\n",
    "\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df = pd.DataFrame(jobs_data)\n",
    "df.to_csv('tryout.csv', index=False)\n",
    "print(\"Job extraction complete. Data saved to 'indeed_jobs.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "face0079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 256 completed.\n",
      "Page 271 completed.\n",
      "Page 286 completed.\n",
      "Page 301 completed.\n",
      "Page 316 completed.\n",
      "Page 331 completed.\n",
      "Page 346 completed.\n",
      "Page 361 completed.\n",
      "Page 376 completed.\n",
      "Page 391 completed.\n",
      "Page 406 completed.\n",
      "Page 421 completed.\n",
      "Page 436 completed.\n",
      "Page 451 completed.\n",
      "Page 466 completed.\n",
      "Page 481 completed.\n",
      "Page 496 completed.\n",
      "Page 511 completed.\n",
      "Page 526 completed.\n",
      "Page 541 completed.\n",
      "Page 556 completed.\n",
      "Page 571 completed.\n",
      "Page 586 completed.\n",
      "Page 601 completed.\n",
      "Page 616 completed.\n",
      "Page 631 completed.\n",
      "Page 646 completed.\n",
      "Page 661 completed.\n",
      "Page 676 completed.\n",
      "Page 691 completed.\n",
      "Page 706 completed.\n",
      "Page 721 completed.\n",
      "Page 736 completed.\n",
      "Page 751 completed.\n",
      "Page 766 completed.\n",
      "Page 781 completed.\n",
      "Page 796 completed.\n",
      "Page 811 completed.\n",
      "Page 826 completed.\n",
      "Page 841 completed.\n",
      "Page 856 completed.\n",
      "Page 871 completed.\n",
      "Page 886 completed.\n",
      "Page 901 completed.\n",
      "Page 916 completed.\n",
      "Page 931 completed.\n",
      "Page 946 completed.\n",
      "Page 961 completed.\n",
      "Page 976 completed.\n",
      "Page 991 completed.\n",
      "Page 1006 completed.\n",
      "Page 1021 completed.\n",
      "Page 1036 completed.\n",
      "Page 1051 completed.\n",
      "Page 1066 completed.\n",
      "Page 1081 completed.\n",
      "Page 1096 completed.\n",
      "Page 1111 completed.\n",
      "Page 1126 completed.\n",
      "Page 1141 completed.\n",
      "Page 1156 completed.\n",
      "Page 1171 completed.\n",
      "Page 1186 completed.\n",
      "Page 1201 completed.\n",
      "Page 1210 completed.\n",
      "Page 1225 completed.\n",
      "Page 1240 completed.\n",
      "Page 1255 completed.\n",
      "Page 1270 completed.\n",
      "Page 1285 completed.\n",
      "Page 1300 completed.\n",
      "Page 1315 completed.\n",
      "Page 1330 completed.\n",
      "Page 1345 completed.\n",
      "Page 1360 completed.\n",
      "Page 1375 completed.\n",
      "Page 1390 completed.\n",
      "Page 1405 completed.\n",
      "Page 1420 completed.\n",
      "Page 1435 completed.\n",
      "Page 1450 completed.\n",
      "Page 1465 completed.\n",
      "Page 1480 completed.\n",
      "Page 1495 completed.\n",
      "Page 1510 completed.\n",
      "Page 1525 completed.\n",
      "Page 1540 completed.\n",
      "Page 1555 completed.\n",
      "Page 1570 completed.\n",
      "Page 1585 completed.\n",
      "Page 1600 completed.\n",
      "Page 1615 completed.\n",
      "Page 1630 completed.\n",
      "Page 1645 completed.\n",
      "Page 1660 completed.\n",
      "Page 1675 completed.\n",
      "Page 1690 completed.\n",
      "Page 1705 completed.\n",
      "Page 1720 completed.\n",
      "Page 1735 completed.\n",
      "Page 1750 completed.\n",
      "Page 1765 completed.\n",
      "Page 1780 completed.\n",
      "Page 1795 completed.\n",
      "Page 1810 completed.\n",
      "Page 1825 completed.\n",
      "Page 1840 completed.\n",
      "Page 1855 completed.\n",
      "Page 1870 completed.\n",
      "Page 1885 completed.\n",
      "Page 1900 completed.\n",
      "Page 1915 completed.\n",
      "Page 1930 completed.\n",
      "Page 1945 completed.\n",
      "Page 1960 completed.\n",
      "Page 1975 completed.\n",
      "Page 1990 completed.\n",
      "Page 2005 completed.\n",
      "Page 2020 completed.\n",
      "Page 2035 completed.\n",
      "Page 2050 completed.\n",
      "Page 2065 completed.\n",
      "Page 2080 completed.\n",
      "Page 2095 completed.\n",
      "Page 2110 completed.\n",
      "Page 2125 completed.\n",
      "Page 2140 completed.\n",
      "Page 2155 completed.\n",
      "Page 2170 completed.\n",
      "Page 2185 completed.\n",
      "Page 2200 completed.\n",
      "Page 2215 completed.\n",
      "Page 2230 completed.\n",
      "Page 2245 completed.\n",
      "Page 2260 completed.\n",
      "Page 2275 completed.\n",
      "Page 2290 completed.\n",
      "Page 2305 completed.\n",
      "Page 2320 completed.\n",
      "Page 2335 completed.\n",
      "Page 2350 completed.\n",
      "Page 2365 completed.\n",
      "Page 2380 completed.\n",
      "Page 2395 completed.\n",
      "Page 2410 completed.\n",
      "Page 2425 completed.\n",
      "Page 2440 completed.\n",
      "Page 2455 completed.\n",
      "Page 2470 completed.\n",
      "Page 2485 completed.\n",
      "Page 2500 completed.\n",
      "Page 2515 completed.\n",
      "Page 2530 completed.\n",
      "Page 2545 completed.\n",
      "Page 2560 completed.\n",
      "Page 2575 completed.\n",
      "Page 2590 completed.\n",
      "Page 2605 completed.\n",
      "Page 2620 completed.\n",
      "Page 2635 completed.\n",
      "Page 2650 completed.\n",
      "Page 2665 completed.\n",
      "Page 2680 completed.\n",
      "Page 2695 completed.\n",
      "Page 2710 completed.\n",
      "Page 2725 completed.\n",
      "Page 2740 completed.\n",
      "Page 2755 completed.\n",
      "Page 2770 completed.\n",
      "Page 2785 completed.\n",
      "Page 2800 completed.\n",
      "Page 2815 completed.\n",
      "Page 2830 completed.\n",
      "Page 2845 completed.\n",
      "Page 2860 completed.\n",
      "Page 2875 completed.\n",
      "Page 2890 completed.\n",
      "Page 2905 completed.\n",
      "Page 2920 completed.\n",
      "Page 2935 completed.\n",
      "Page 2950 completed.\n",
      "Page 2965 completed.\n",
      "Page 2980 completed.\n",
      "Page 2995 completed.\n",
      "Page 3010 completed.\n",
      "Page 3025 completed.\n",
      "Page 3040 completed.\n",
      "Page 3055 completed.\n",
      "Page 3070 completed.\n",
      "Page 3085 completed.\n",
      "Page 3100 completed.\n",
      "Page 3115 completed.\n",
      "Page 3130 completed.\n",
      "Page 3145 completed.\n",
      "Page 3160 completed.\n",
      "Page 3175 completed.\n",
      "Page 3190 completed.\n",
      "Page 3205 completed.\n",
      "Page 3220 completed.\n",
      "Page 3235 completed.\n",
      "Job extraction complete. Data saved to 'indeed_jobs.csv'.\n"
     ]
    }
   ],
   "source": [
    "jobs_data = []\n",
    "\n",
    "\n",
    "for i in range(0,2000,10):\n",
    "    driver.get('https://www.indeed.com/jobs?q=sustainability%20&l=united%20states&start='+str(i))\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "\n",
    "    #https://www.indeed.com/jobs?q=sustainability++jobs&l=USA&vjk=0cdb55029eb47b25\n",
    "\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.job_seen_beacon')))\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "        break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.findAll('div', class_='job_seen_beacon')\n",
    "\n",
    "    if not jobs:\n",
    "        print(\"No more jobs found.\")\n",
    "        break\n",
    "\n",
    "    for job in jobs:\n",
    "        data = {}\n",
    "        try:\n",
    "            data['Title'] = job.find('h2', class_='jobTitle').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Title'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Company'] = job.find('span', class_=\"css-92r8pb\").text.strip()\n",
    "        except AttributeError:\n",
    "            data['Company'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Location'] = job.find('div', class_='company_location').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Location'] = \"Not listed\"\n",
    "\n",
    "        skills_section = job.find('div', class_='jobsearch-SkillsWrapper')\n",
    "        data['Skills'] = ', '.join([skill.text.strip() for skill in skills_section.find_all('span', class_='jobsearch-SkillTag')]) if skills_section else \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Salary'] = job.find('div', class_='css-1cvo3fd').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Salary'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            jobDescriptionText = job.find('div', class_='jobsearch-jobDescriptionText')\n",
    "            data['Description'] = jobDescriptionText.text.strip() if jobDescriptionText else \"Not listed\"\n",
    "        except AttributeError:\n",
    "            data['Description'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            job_link = job.find('a', class_='jcs-JobTitle')['href']\n",
    "            data['Apply Link'] = 'https://www.indeed.com' + job_link\n",
    "        except TypeError:\n",
    "            data['Apply Link'] = \"Not listed\"\n",
    "\n",
    "        jobs_data.append(data)\n",
    "        page += 1\n",
    "    print(f\"Page {page + 1} completed.\")\n",
    "\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df = pd.DataFrame(jobs_data)\n",
    "df.to_csv('indeed_jobs_final.csv', index=False)\n",
    "print(\"Job extraction complete. Data saved to 'indeed_jobs.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d04ebdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a6e755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd819386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d01a0113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 16 completed.\n",
      "Job extraction complete. Data saved to 'indeed_jobs.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "jobs_data = []\n",
    "max_pages = 5  # Set a maximum number of pages to scrape\n",
    "page = 0\n",
    "\n",
    "while page < max_pages:\n",
    "    url = f\"https://www.indeed.com/jobs?q=sustainability++jobs&l=USA&vjk={page * 10}\"\n",
    "    driver.get(url)\n",
    "\n",
    "    #https://www.indeed.com/jobs?q=sustainability++jobs&l=USA&vjk=0cdb55029eb47b25\n",
    "\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.job_seen_beacon')))\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "        break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.findAll('div', class_='job_seen_beacon')\n",
    "\n",
    "    if not jobs:\n",
    "        print(\"No more jobs found.\")\n",
    "        break\n",
    "\n",
    "    for job in jobs:\n",
    "        data = {}\n",
    "        try:\n",
    "            data['Title'] = job.find('h2', class_='jobTitle').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Title'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Company'] = job.find('span', class_=\"css-92r8pb\").text.strip()\n",
    "        except AttributeError:\n",
    "            data['Company'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Location'] = job.find('div', class_='company_location').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Location'] = \"Not listed\"\n",
    "\n",
    "        skills_section = job.find('div', class_='jobsearch-SkillsWrapper')\n",
    "        data['Skills'] = ', '.join([skill.text.strip() for skill in skills_section.find_all('span', class_='jobsearch-SkillTag')]) if skills_section else \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Salary'] = job.find('div', class_='css-1cvo3fd').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Salary'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            jobDescriptionText = job.find('div', class_='jobsearch-jobDescriptionText')\n",
    "            data['Description'] = jobDescriptionText.text.strip() if jobDescriptionText else \"Not listed\"\n",
    "        except AttributeError:\n",
    "            data['Description'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            job_link = job.find('a', class_='jcs-JobTitle')['href']\n",
    "            data['Apply Link'] = 'https://www.indeed.com' + job_link\n",
    "        except TypeError:\n",
    "            data['Apply Link'] = \"Not listed\"\n",
    "\n",
    "        jobs_data.append(data)\n",
    "        page += 1\n",
    "    print(f\"Page {page + 1} completed.\")\n",
    "\n",
    "# Check for the 'Next' button - this may vary depending on Indeed's page structure\n",
    "next_button = soup.find('a', {'aria-label': 'Next'})\n",
    "if next_button and 'href' in next_button.attrs:\n",
    "  # Indeed uses a 'start' parameter to paginate\n",
    "  params['start'] += 10\n",
    "\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df = pd.DataFrame(jobs_data)\n",
    "df.to_csv('indeed_jobs_one.csv', index=False)\n",
    "print(\"Job extraction complete. Data saved to 'indeed_jobs.csv'.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d898d7bc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41153cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd16afdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a326505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420a1027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "050d5bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 16 completed.\n",
      "Job extraction complete. Data saved to 'indeed_jobs.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "jobs_data = []\n",
    "max_pages = 5  # Set a maximum number of pages to scrape\n",
    "page = 0\n",
    "\n",
    "while page < max_pages:\n",
    "    url = f\"https://www.indeed.com/jobs?q=sustainability++jobs&l=USA&vjk={page * 10}\"\n",
    "    driver.get(url)\n",
    "\n",
    "    #https://www.indeed.com/jobs?q=sustainability++jobs&l=USA&vjk=0cdb55029eb47b25\n",
    "\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.job_seen_beacon')))\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "        break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.findAll('div', class_='job_seen_beacon')\n",
    "\n",
    "    if not jobs:\n",
    "        print(\"No more jobs found.\")\n",
    "        break\n",
    "\n",
    "    for job in jobs:\n",
    "        data = {}\n",
    "        try:\n",
    "            data['Title'] = job.find('h2', class_='jobTitle').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Title'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Company'] = job.find('span', class_=\"css-92r8pb\").text.strip()\n",
    "        except AttributeError:\n",
    "            data['Company'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Location'] = job.find('div', class_='company_location').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Location'] = \"Not listed\"\n",
    "\n",
    "        skills_section = job.find('div', class_='jobsearch-SkillsWrapper')\n",
    "        data['Skills'] = ', '.join([skill.text.strip() for skill in skills_section.find_all('span', class_='jobsearch-SkillTag')]) if skills_section else \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Salary'] = job.find('div', class_='css-1cvo3fd').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Salary'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            jobDescriptionText = job.find('div', class_='jobsearch-jobDescriptionText')\n",
    "            data['Description'] = jobDescriptionText.text.strip() if jobDescriptionText else \"Not listed\"\n",
    "        except AttributeError:\n",
    "            data['Description'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            job_link = job.find('a', class_='jcs-JobTitle')['href']\n",
    "            data['Apply Link'] = 'https://www.indeed.com' + job_link\n",
    "        except TypeError:\n",
    "            data['Apply Link'] = \"Not listed\"\n",
    "\n",
    "        jobs_data.append(data)\n",
    "        page += 1\n",
    "    print(f\"Page {page + 1} completed.\")\n",
    "    \n",
    "page += 1\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df = pd.DataFrame(jobs_data)\n",
    "df.to_csv('indeed_jobs_one.csv', index=False)\n",
    "print(\"Job extraction complete. Data saved to 'indeed_jobs.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "628eba17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 16 completed.\n",
      "Job extraction complete. Data saved to 'indeed_jobs.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "jobs_data = []\n",
    "max_pages = 5  # Set a maximum number of pages to scrape\n",
    "page = 0\n",
    "\n",
    "while page < max_pages:\n",
    "    url = f\"https://www.indeed.com/jobs?q=sustainability++jobs&l=USA&vjk={page * 10}\"\n",
    "    driver.get(url)\n",
    "\n",
    "    #https://www.indeed.com/jobs?q=sustainability++jobs&l=USA&vjk=0cdb55029eb47b25\n",
    "\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.job_seen_beacon')))\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "        break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.findAll('div', class_='job_seen_beacon')\n",
    "\n",
    "    if not jobs:\n",
    "        print(\"No more jobs found.\")\n",
    "        break\n",
    "\n",
    "    for job in jobs:\n",
    "        data = {}\n",
    "        try:\n",
    "            data['Title'] = job.find('h2', class_='jobTitle').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Title'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Company'] = job.find('span', class_=\"css-92r8pb\").text.strip()\n",
    "        except AttributeError:\n",
    "            data['Company'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Location'] = job.find('div', class_='company_location').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Location'] = \"Not listed\"\n",
    "\n",
    "        skills_section = job.find('div', class_='jobsearch-SkillsWrapper')\n",
    "        data['Skills'] = ', '.join([skill.text.strip() for skill in skills_section.find_all('span', class_='jobsearch-SkillTag')]) if skills_section else \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Salary'] = job.find('div', class_='css-1cvo3fd').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Salary'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            jobDescriptionText = job.find('div', class_='jobsearch-jobDescriptionText')\n",
    "            data['Description'] = jobDescriptionText.text.strip() if jobDescriptionText else \"Not listed\"\n",
    "        except AttributeError:\n",
    "            data['Description'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            job_link = job.find('a', class_='jcs-JobTitle')['href']\n",
    "            data['Apply Link'] = 'https://www.indeed.com' + job_link\n",
    "        except TypeError:\n",
    "            data['Apply Link'] = \"Not listed\"\n",
    "\n",
    "        jobs_data.append(data)\n",
    "        page += 1\n",
    "    print(f\"Page {page + 1} completed.\")\n",
    "    \n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df = pd.DataFrame(jobs_data)\n",
    "df.to_csv('indeed_jobs.csv', index=False)\n",
    "print(\"Job extraction complete. Data saved to 'indeed_jobs.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53b7c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfa85c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66b0f516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 completed.\n",
      "Page 2 completed.\n",
      "Page 3 completed.\n",
      "Page 4 completed.\n",
      "Page 5 completed.\n",
      "Job extraction complete. Data saved to 'indeed_jobs.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "jobs_data = []\n",
    "max_pages = 5  # Set a maximum number of pages to scrape\n",
    "page = 0\n",
    "\n",
    "while page < max_pages:\n",
    "    url = f\"https://www.indeed.com/jobs?q=sustainability++jobs&l=USA&vjk={page * 10}\"\n",
    "    driver.get(url)\n",
    "\n",
    "    #https://www.indeed.com/jobs?q=sustainability++jobs&l=USA&vjk=0cdb55029eb47b25\n",
    "\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.job_seen_beacon')))\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "        break\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.findAll('div', class_='job_seen_beacon')\n",
    "\n",
    "    if not jobs:\n",
    "        print(\"No more jobs found.\")\n",
    "        break\n",
    "\n",
    "    for job in jobs:\n",
    "        data = {}\n",
    "        try:\n",
    "            data['Title'] = job.find('h2', class_='jobTitle').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Title'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Company'] = job.find('span', class_=\"css-92r8pb\").text.strip()\n",
    "        except AttributeError:\n",
    "            data['Company'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Location'] = job.find('div', class_='company_location').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Location'] = \"Not listed\"\n",
    "\n",
    "        skills_section = job.find('div', class_='jobsearch-SkillsWrapper')\n",
    "        data['Skills'] = ', '.join([skill.text.strip() for skill in skills_section.find_all('span', class_='jobsearch-SkillTag')]) if skills_section else \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            data['Salary'] = job.find('div', class_='css-1cvo3fd').text.strip()\n",
    "        except AttributeError:\n",
    "            data['Salary'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            jobDescriptionText = job.find('div', class_='jobsearch-jobDescriptionText')\n",
    "            data['Description'] = jobDescriptionText.text.strip() if jobDescriptionText else \"Not listed\"\n",
    "        except AttributeError:\n",
    "            data['Description'] = \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            job_link = job.find('a', class_='jcs-JobTitle')['href']\n",
    "            data['Apply Link'] = 'https://www.indeed.com' + job_link\n",
    "        except TypeError:\n",
    "            data['Apply Link'] = \"Not listed\"\n",
    "\n",
    "        jobs_data.append(data)\n",
    "\n",
    "    print(f\"Page {page + 1} completed.\")\n",
    "    page += 1\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df = pd.DataFrame(jobs_data)\n",
    "df.to_csv('indeed_jobs.csv', index=False)\n",
    "print(\"Job extraction complete. Data saved to 'indeed_jobs.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e83cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9923a0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8409814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03a282b1-6024-4953-be1d-f94783fbbc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.indeed.com/jobs?q=sustainability+jobs&l=USA&sc=0kf%3Aexplvl%28ENTRY_LEVEL%29%3B&vjk=93fbcedf8267811e=0\n",
      "Page 1 done.\n",
      "Fetching: https://www.indeed.com/jobs?q=sustainability+jobs&l=USA&sc=0kf%3Aexplvl%28ENTRY_LEVEL%29%3B&vjk=93fbcedf8267811e=12\n",
      "Page 2 done.\n",
      "Fetching: https://www.indeed.com/jobs?q=sustainability+jobs&l=USA&sc=0kf%3Aexplvl%28ENTRY_LEVEL%29%3B&vjk=93fbcedf8267811e=24\n",
      "Page 3 done.\n",
      "Data saved to CSV file successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize jobs_data list\n",
    "jobs_data = []\n",
    "\n",
    "# Set maximum number of pages\n",
    "max_pages = 3\n",
    "\n",
    "# Loop through each page until max_pages is reached\n",
    "page = 0\n",
    "while page < max_pages:\n",
    "    page_val = page * 12  # Pagination step\n",
    "    url = f\"https://www.indeed.com/jobs?q=sustainability+jobs&l=USA&sc=0kf%3Aexplvl%28ENTRY_LEVEL%29%3B&vjk=93fbcedf8267811e={page_val}\"\n",
    "    print(\"Fetching:\", url)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the job listings to be loaded\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.job_seen_beacon')))\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "        break\n",
    "\n",
    "    # Extract job listings\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.findAll('div', class_='job_seen_beacon')\n",
    "\n",
    "    if not jobs:\n",
    "        print(\"No more jobs found. Ending search.\")\n",
    "        break\n",
    "\n",
    "    # Loop through each job listing\n",
    "    for job in jobs:\n",
    "        data = {}\n",
    "        data['Apply_Link'] = 'Not listed'\n",
    "\n",
    "        try:\n",
    "            title = job.find('h2', class_='jobTitle').text.strip()\n",
    "        except AttributeError:\n",
    "            title = \"Not listed\"\n",
    "        data['Title'] = title\n",
    "\n",
    "        try:\n",
    "            company = job.find('span', class_=\"css-92r8pb\").text.strip()\n",
    "        except AttributeError:\n",
    "            company = \"Not listed\"\n",
    "        data['Company'] = company\n",
    "\n",
    "        try:\n",
    "            location = job.find('div', class_='css-1p0sjhy').text.strip()\n",
    "        except AttributeError:\n",
    "            location = \"Not listed\"\n",
    "        data['Location'] = location\n",
    "\n",
    "        salary = job.find('div', class_='css-1cvo3fd')\n",
    "        data['Salary'] = salary.text.strip() if salary else \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            wait.until(EC.presence_of_element_located((By.ID, 'jobDescriptionText')))\n",
    "            detailed_description = driver.find_element(By.ID, 'jobDescriptionText').text.strip()\n",
    "        except (TimeoutException, NoSuchElementException):\n",
    "            detailed_description = \"Not listed\"\n",
    "\n",
    "        data['Description'] = detailed_description\n",
    "        jobs_data.append(data)\n",
    "\n",
    "    print(f\"Page {page + 1} done.\")\n",
    "    page += 1  # Move to the next page\n",
    "\n",
    "# Close the WebDriver after finishing\n",
    "driver.quit()\n",
    "\n",
    "# Save data to CSV file\n",
    "csv_file_path = \"242024.csv\"\n",
    "with open(csv_file_path, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "    fieldnames = ['Title', 'Company', 'Location', 'Salary', 'Description', 'Apply_Link']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for job_data in jobs_data:\n",
    "        writer.writerow(job_data)\n",
    "\n",
    "print(\"Data saved to CSV file successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db655967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.indeed.com/jobs?q=sustainability+jobs&l=USA&sc=0kf%3Aexplvl%28ENTRY_LEVEL%29%3B&vjk=93fbcedf8267811e=0\n",
      "Page 1 done.\n",
      "Fetching: https://www.indeed.com/jobs?q=sustainability+jobs&l=USA&sc=0kf%3Aexplvl%28ENTRY_LEVEL%29%3B&vjk=93fbcedf8267811e=12\n",
      "Page 2 done.\n",
      "Fetching: https://www.indeed.com/jobs?q=sustainability+jobs&l=USA&sc=0kf%3Aexplvl%28ENTRY_LEVEL%29%3B&vjk=93fbcedf8267811e=24\n",
      "Page 3 done.\n",
      "Data saved to CSV file successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize jobs_data list\n",
    "jobs_data = []\n",
    "\n",
    "# Set maximum number of pages\n",
    "max_pages = 3\n",
    "\n",
    "# Loop through each page until max_pages is reached\n",
    "page = 0\n",
    "while page < max_pages:\n",
    "    page_val = page * 12  # Pagination step\n",
    "    url = f\"https://www.indeed.com/jobs?q=sustainability+jobs&l=USA&sc=0kf%3Aexplvl%28ENTRY_LEVEL%29%3B&vjk=93fbcedf8267811e={page_val}\"\n",
    "    print(\"Fetching:\", url)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the job listings to be loaded\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.job_seen_beacon')))\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for page to load\")\n",
    "        break\n",
    "\n",
    "    # Extract job listings\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.findAll('div', class_='job_seen_beacon')\n",
    "\n",
    "    if not jobs:\n",
    "        print(\"No more jobs found. Ending search.\")\n",
    "        break\n",
    "\n",
    "    # Loop through each job listing\n",
    "    for job in jobs:\n",
    "        data = {}\n",
    "        data['Apply_Link'] = 'Not listed'\n",
    "\n",
    "        try:\n",
    "            title = job.find('h2', class_='jobTitle').text.strip()\n",
    "        except AttributeError:\n",
    "            title = \"Not listed\"\n",
    "        data['Title'] = title\n",
    "\n",
    "        try:\n",
    "            company = job.find('span', class_=\"css-92r8pb\").text.strip()\n",
    "        except AttributeError:\n",
    "            company = \"Not listed\"\n",
    "        data['Company'] = company\n",
    "\n",
    "        try:\n",
    "            location = job.find('div', class_='css-1p0sjhy').text.strip()\n",
    "        except AttributeError:\n",
    "            location = \"Not listed\"\n",
    "        data['Location'] = location\n",
    "\n",
    "        salary = job.find('div', class_='css-1cvo3fd')\n",
    "        data['Salary'] = salary.text.strip() if salary else \"Not listed\"\n",
    "\n",
    "        try:\n",
    "            wait.until(EC.presence_of_element_located((By.ID, 'jobDescriptionText')))\n",
    "            detailed_description = driver.find_element(By.ID, 'jobDescriptionText').text.strip()\n",
    "        except (TimeoutException, NoSuchElementException):\n",
    "            detailed_description = \"Not listed\"\n",
    "\n",
    "        data['Description'] = detailed_description\n",
    "        jobs_data.append(data)\n",
    "\n",
    "    print(f\"Page {page + 1} done.\")\n",
    "    page += 1  # Move to the next page\n",
    "\n",
    "# Close the WebDriver after finishing\n",
    "driver.quit()\n",
    "\n",
    "# Save data to CSV file\n",
    "csv_file_path = \"242024.csv\"\n",
    "with open(csv_file_path, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "    fieldnames = ['Title', 'Company', 'Location', 'Salary', 'Description', 'Apply_Link']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for job_data in jobs_data:\n",
    "        writer.writerow(job_data)\n",
    "\n",
    "print(\"Data saved to CSV file successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
